
================================================================================
COMPREHENSIVE MODEL ANALYZER
Showing ALL keys without truncation
================================================================================
================================================================================
MODEL ANALYSIS: fluxlora.safetensors
Full path: fluxlora.safetensors
================================================================================

Total keys: 912
Total parameters: 19,145,008

================================================================================
FIRST 100 KEYS (use --detailed for all 912 keys)
================================================================================
   1. lora_unet_double_blocks_0_img_attn_proj.alpha: torch.Size([]) (torch.bfloat16)
   2. lora_unet_double_blocks_0_img_attn_proj.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   3. lora_unet_double_blocks_0_img_attn_proj.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
   4. lora_unet_double_blocks_0_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   5. lora_unet_double_blocks_0_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   6. lora_unet_double_blocks_0_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   7. lora_unet_double_blocks_0_img_mlp_0.alpha: torch.Size([]) (torch.bfloat16)
   8. lora_unet_double_blocks_0_img_mlp_0.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   9. lora_unet_double_blocks_0_img_mlp_0.lora_up.weight: torch.Size([12288, 4]) (torch.bfloat16)
  10. lora_unet_double_blocks_0_img_mlp_2.alpha: torch.Size([]) (torch.bfloat16)
  11. lora_unet_double_blocks_0_img_mlp_2.lora_down.weight: torch.Size([4, 12288]) (torch.bfloat16)
  12. lora_unet_double_blocks_0_img_mlp_2.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  13. lora_unet_double_blocks_0_img_mod_lin.alpha: torch.Size([]) (torch.bfloat16)
  14. lora_unet_double_blocks_0_img_mod_lin.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  15. lora_unet_double_blocks_0_img_mod_lin.lora_up.weight: torch.Size([18432, 4]) (torch.bfloat16)
  16. lora_unet_double_blocks_0_txt_attn_proj.alpha: torch.Size([]) (torch.bfloat16)
  17. lora_unet_double_blocks_0_txt_attn_proj.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  18. lora_unet_double_blocks_0_txt_attn_proj.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  19. lora_unet_double_blocks_0_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  20. lora_unet_double_blocks_0_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  21. lora_unet_double_blocks_0_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  22. lora_unet_double_blocks_0_txt_mlp_0.alpha: torch.Size([]) (torch.bfloat16)
  23. lora_unet_double_blocks_0_txt_mlp_0.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  24. lora_unet_double_blocks_0_txt_mlp_0.lora_up.weight: torch.Size([12288, 4]) (torch.bfloat16)
  25. lora_unet_double_blocks_0_txt_mlp_2.alpha: torch.Size([]) (torch.bfloat16)
  26. lora_unet_double_blocks_0_txt_mlp_2.lora_down.weight: torch.Size([4, 12288]) (torch.bfloat16)
  27. lora_unet_double_blocks_0_txt_mlp_2.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  28. lora_unet_double_blocks_0_txt_mod_lin.alpha: torch.Size([]) (torch.bfloat16)
  29. lora_unet_double_blocks_0_txt_mod_lin.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  30. lora_unet_double_blocks_0_txt_mod_lin.lora_up.weight: torch.Size([18432, 4]) (torch.bfloat16)
  31. lora_unet_double_blocks_10_img_attn_proj.alpha: torch.Size([]) (torch.bfloat16)
  32. lora_unet_double_blocks_10_img_attn_proj.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  33. lora_unet_double_blocks_10_img_attn_proj.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  34. lora_unet_double_blocks_10_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  35. lora_unet_double_blocks_10_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  36. lora_unet_double_blocks_10_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  37. lora_unet_double_blocks_10_img_mlp_0.alpha: torch.Size([]) (torch.bfloat16)
  38. lora_unet_double_blocks_10_img_mlp_0.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  39. lora_unet_double_blocks_10_img_mlp_0.lora_up.weight: torch.Size([12288, 4]) (torch.bfloat16)
  40. lora_unet_double_blocks_10_img_mlp_2.alpha: torch.Size([]) (torch.bfloat16)
  41. lora_unet_double_blocks_10_img_mlp_2.lora_down.weight: torch.Size([4, 12288]) (torch.bfloat16)
  42. lora_unet_double_blocks_10_img_mlp_2.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  43. lora_unet_double_blocks_10_img_mod_lin.alpha: torch.Size([]) (torch.bfloat16)
  44. lora_unet_double_blocks_10_img_mod_lin.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  45. lora_unet_double_blocks_10_img_mod_lin.lora_up.weight: torch.Size([18432, 4]) (torch.bfloat16)
  46. lora_unet_double_blocks_10_txt_attn_proj.alpha: torch.Size([]) (torch.bfloat16)
  47. lora_unet_double_blocks_10_txt_attn_proj.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  48. lora_unet_double_blocks_10_txt_attn_proj.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  49. lora_unet_double_blocks_10_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  50. lora_unet_double_blocks_10_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  51. lora_unet_double_blocks_10_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  52. lora_unet_double_blocks_10_txt_mlp_0.alpha: torch.Size([]) (torch.bfloat16)
  53. lora_unet_double_blocks_10_txt_mlp_0.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  54. lora_unet_double_blocks_10_txt_mlp_0.lora_up.weight: torch.Size([12288, 4]) (torch.bfloat16)
  55. lora_unet_double_blocks_10_txt_mlp_2.alpha: torch.Size([]) (torch.bfloat16)
  56. lora_unet_double_blocks_10_txt_mlp_2.lora_down.weight: torch.Size([4, 12288]) (torch.bfloat16)
  57. lora_unet_double_blocks_10_txt_mlp_2.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  58. lora_unet_double_blocks_10_txt_mod_lin.alpha: torch.Size([]) (torch.bfloat16)
  59. lora_unet_double_blocks_10_txt_mod_lin.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  60. lora_unet_double_blocks_10_txt_mod_lin.lora_up.weight: torch.Size([18432, 4]) (torch.bfloat16)
  61. lora_unet_double_blocks_11_img_attn_proj.alpha: torch.Size([]) (torch.bfloat16)
  62. lora_unet_double_blocks_11_img_attn_proj.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  63. lora_unet_double_blocks_11_img_attn_proj.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  64. lora_unet_double_blocks_11_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  65. lora_unet_double_blocks_11_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  66. lora_unet_double_blocks_11_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  67. lora_unet_double_blocks_11_img_mlp_0.alpha: torch.Size([]) (torch.bfloat16)
  68. lora_unet_double_blocks_11_img_mlp_0.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  69. lora_unet_double_blocks_11_img_mlp_0.lora_up.weight: torch.Size([12288, 4]) (torch.bfloat16)
  70. lora_unet_double_blocks_11_img_mlp_2.alpha: torch.Size([]) (torch.bfloat16)
  71. lora_unet_double_blocks_11_img_mlp_2.lora_down.weight: torch.Size([4, 12288]) (torch.bfloat16)
  72. lora_unet_double_blocks_11_img_mlp_2.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  73. lora_unet_double_blocks_11_img_mod_lin.alpha: torch.Size([]) (torch.bfloat16)
  74. lora_unet_double_blocks_11_img_mod_lin.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  75. lora_unet_double_blocks_11_img_mod_lin.lora_up.weight: torch.Size([18432, 4]) (torch.bfloat16)
  76. lora_unet_double_blocks_11_txt_attn_proj.alpha: torch.Size([]) (torch.bfloat16)
  77. lora_unet_double_blocks_11_txt_attn_proj.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  78. lora_unet_double_blocks_11_txt_attn_proj.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  79. lora_unet_double_blocks_11_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  80. lora_unet_double_blocks_11_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  81. lora_unet_double_blocks_11_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  82. lora_unet_double_blocks_11_txt_mlp_0.alpha: torch.Size([]) (torch.bfloat16)
  83. lora_unet_double_blocks_11_txt_mlp_0.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  84. lora_unet_double_blocks_11_txt_mlp_0.lora_up.weight: torch.Size([12288, 4]) (torch.bfloat16)
  85. lora_unet_double_blocks_11_txt_mlp_2.alpha: torch.Size([]) (torch.bfloat16)
  86. lora_unet_double_blocks_11_txt_mlp_2.lora_down.weight: torch.Size([4, 12288]) (torch.bfloat16)
  87. lora_unet_double_blocks_11_txt_mlp_2.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  88. lora_unet_double_blocks_11_txt_mod_lin.alpha: torch.Size([]) (torch.bfloat16)
  89. lora_unet_double_blocks_11_txt_mod_lin.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  90. lora_unet_double_blocks_11_txt_mod_lin.lora_up.weight: torch.Size([18432, 4]) (torch.bfloat16)
  91. lora_unet_double_blocks_12_img_attn_proj.alpha: torch.Size([]) (torch.bfloat16)
  92. lora_unet_double_blocks_12_img_attn_proj.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  93. lora_unet_double_blocks_12_img_attn_proj.lora_up.weight: torch.Size([3072, 4]) (torch.bfloat16)
  94. lora_unet_double_blocks_12_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  95. lora_unet_double_blocks_12_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  96. lora_unet_double_blocks_12_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  97. lora_unet_double_blocks_12_img_mlp_0.alpha: torch.Size([]) (torch.bfloat16)
  98. lora_unet_double_blocks_12_img_mlp_0.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  99. lora_unet_double_blocks_12_img_mlp_0.lora_up.weight: torch.Size([12288, 4]) (torch.bfloat16)
 100. lora_unet_double_blocks_12_img_mlp_2.alpha: torch.Size([]) (torch.bfloat16)

... and 812 more keys (use --detailed to see all)

================================================================================
ATTENTION KEYS ANALYSIS
================================================================================
NO ATTENTION KEYS FOUND!

================================================================================
QKV PATTERN ANALYSIS
================================================================================

attn qkv (.*attn.*qkv.*): 114 keys
    1. lora_unet_double_blocks_0_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
    2. lora_unet_double_blocks_0_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
    3. lora_unet_double_blocks_0_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
    4. lora_unet_double_blocks_0_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
    5. lora_unet_double_blocks_0_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
    6. lora_unet_double_blocks_0_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
    7. lora_unet_double_blocks_10_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
    8. lora_unet_double_blocks_10_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
    9. lora_unet_double_blocks_10_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   10. lora_unet_double_blocks_10_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   11. lora_unet_double_blocks_10_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   12. lora_unet_double_blocks_10_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   13. lora_unet_double_blocks_11_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   14. lora_unet_double_blocks_11_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   15. lora_unet_double_blocks_11_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   16. lora_unet_double_blocks_11_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   17. lora_unet_double_blocks_11_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   18. lora_unet_double_blocks_11_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   19. lora_unet_double_blocks_12_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   20. lora_unet_double_blocks_12_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   21. lora_unet_double_blocks_12_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   22. lora_unet_double_blocks_12_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   23. lora_unet_double_blocks_12_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   24. lora_unet_double_blocks_12_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   25. lora_unet_double_blocks_13_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   26. lora_unet_double_blocks_13_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   27. lora_unet_double_blocks_13_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   28. lora_unet_double_blocks_13_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   29. lora_unet_double_blocks_13_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   30. lora_unet_double_blocks_13_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   31. lora_unet_double_blocks_14_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   32. lora_unet_double_blocks_14_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   33. lora_unet_double_blocks_14_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   34. lora_unet_double_blocks_14_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   35. lora_unet_double_blocks_14_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   36. lora_unet_double_blocks_14_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   37. lora_unet_double_blocks_15_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   38. lora_unet_double_blocks_15_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   39. lora_unet_double_blocks_15_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   40. lora_unet_double_blocks_15_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   41. lora_unet_double_blocks_15_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   42. lora_unet_double_blocks_15_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   43. lora_unet_double_blocks_16_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   44. lora_unet_double_blocks_16_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   45. lora_unet_double_blocks_16_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   46. lora_unet_double_blocks_16_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   47. lora_unet_double_blocks_16_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   48. lora_unet_double_blocks_16_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   49. lora_unet_double_blocks_17_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   50. lora_unet_double_blocks_17_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   51. lora_unet_double_blocks_17_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   52. lora_unet_double_blocks_17_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   53. lora_unet_double_blocks_17_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   54. lora_unet_double_blocks_17_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   55. lora_unet_double_blocks_18_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   56. lora_unet_double_blocks_18_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   57. lora_unet_double_blocks_18_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   58. lora_unet_double_blocks_18_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   59. lora_unet_double_blocks_18_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   60. lora_unet_double_blocks_18_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   61. lora_unet_double_blocks_1_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   62. lora_unet_double_blocks_1_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   63. lora_unet_double_blocks_1_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   64. lora_unet_double_blocks_1_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   65. lora_unet_double_blocks_1_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   66. lora_unet_double_blocks_1_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   67. lora_unet_double_blocks_2_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   68. lora_unet_double_blocks_2_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   69. lora_unet_double_blocks_2_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   70. lora_unet_double_blocks_2_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   71. lora_unet_double_blocks_2_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   72. lora_unet_double_blocks_2_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   73. lora_unet_double_blocks_3_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   74. lora_unet_double_blocks_3_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   75. lora_unet_double_blocks_3_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   76. lora_unet_double_blocks_3_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   77. lora_unet_double_blocks_3_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   78. lora_unet_double_blocks_3_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   79. lora_unet_double_blocks_4_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   80. lora_unet_double_blocks_4_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   81. lora_unet_double_blocks_4_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   82. lora_unet_double_blocks_4_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   83. lora_unet_double_blocks_4_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   84. lora_unet_double_blocks_4_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   85. lora_unet_double_blocks_5_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   86. lora_unet_double_blocks_5_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   87. lora_unet_double_blocks_5_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   88. lora_unet_double_blocks_5_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   89. lora_unet_double_blocks_5_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   90. lora_unet_double_blocks_5_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   91. lora_unet_double_blocks_6_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   92. lora_unet_double_blocks_6_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   93. lora_unet_double_blocks_6_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   94. lora_unet_double_blocks_6_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   95. lora_unet_double_blocks_6_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   96. lora_unet_double_blocks_6_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
   97. lora_unet_double_blocks_7_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
   98. lora_unet_double_blocks_7_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
   99. lora_unet_double_blocks_7_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  100. lora_unet_double_blocks_7_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  101. lora_unet_double_blocks_7_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  102. lora_unet_double_blocks_7_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  103. lora_unet_double_blocks_8_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  104. lora_unet_double_blocks_8_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  105. lora_unet_double_blocks_8_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  106. lora_unet_double_blocks_8_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  107. lora_unet_double_blocks_8_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  108. lora_unet_double_blocks_8_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  109. lora_unet_double_blocks_9_img_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  110. lora_unet_double_blocks_9_img_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  111. lora_unet_double_blocks_9_img_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)
  112. lora_unet_double_blocks_9_txt_attn_qkv.alpha: torch.Size([]) (torch.bfloat16)
  113. lora_unet_double_blocks_9_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072]) (torch.bfloat16)
  114. lora_unet_double_blocks_9_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4]) (torch.bfloat16)

================================================================================
MODEL TYPE DETECTION
================================================================================
Detected model type: LoRA adapter (standalone)

================================================================================
QKV LAYER DETAILED ANALYSIS
================================================================================
No standard QKV layers found in first 32 layers.

Searching for separate Q, K, V weights...

================================================================================
LORA/ADAPTER PARAMETER DETECTION
================================================================================
Found 608 explicit LoRA/Adapter keys:
   1. lora_unet_double_blocks_0_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
   2. lora_unet_double_blocks_0_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
   3. lora_unet_double_blocks_0_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
   4. lora_unet_double_blocks_0_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
   5. lora_unet_double_blocks_0_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
   6. lora_unet_double_blocks_0_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
   7. lora_unet_double_blocks_0_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
   8. lora_unet_double_blocks_0_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
   9. lora_unet_double_blocks_0_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
  10. lora_unet_double_blocks_0_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
  11. lora_unet_double_blocks_0_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
  12. lora_unet_double_blocks_0_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
  13. lora_unet_double_blocks_0_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
  14. lora_unet_double_blocks_0_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
  15. lora_unet_double_blocks_0_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
  16. lora_unet_double_blocks_0_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
  17. lora_unet_double_blocks_0_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
  18. lora_unet_double_blocks_0_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
  19. lora_unet_double_blocks_0_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
  20. lora_unet_double_blocks_0_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
  21. lora_unet_double_blocks_10_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
  22. lora_unet_double_blocks_10_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
  23. lora_unet_double_blocks_10_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
  24. lora_unet_double_blocks_10_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
  25. lora_unet_double_blocks_10_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
  26. lora_unet_double_blocks_10_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
  27. lora_unet_double_blocks_10_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
  28. lora_unet_double_blocks_10_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
  29. lora_unet_double_blocks_10_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
  30. lora_unet_double_blocks_10_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
  31. lora_unet_double_blocks_10_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
  32. lora_unet_double_blocks_10_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
  33. lora_unet_double_blocks_10_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
  34. lora_unet_double_blocks_10_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
  35. lora_unet_double_blocks_10_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
  36. lora_unet_double_blocks_10_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
  37. lora_unet_double_blocks_10_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
  38. lora_unet_double_blocks_10_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
  39. lora_unet_double_blocks_10_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
  40. lora_unet_double_blocks_10_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
  41. lora_unet_double_blocks_11_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
  42. lora_unet_double_blocks_11_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
  43. lora_unet_double_blocks_11_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
  44. lora_unet_double_blocks_11_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
  45. lora_unet_double_blocks_11_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
  46. lora_unet_double_blocks_11_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
  47. lora_unet_double_blocks_11_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
  48. lora_unet_double_blocks_11_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
  49. lora_unet_double_blocks_11_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
  50. lora_unet_double_blocks_11_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
  51. lora_unet_double_blocks_11_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
  52. lora_unet_double_blocks_11_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
  53. lora_unet_double_blocks_11_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
  54. lora_unet_double_blocks_11_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
  55. lora_unet_double_blocks_11_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
  56. lora_unet_double_blocks_11_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
  57. lora_unet_double_blocks_11_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
  58. lora_unet_double_blocks_11_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
  59. lora_unet_double_blocks_11_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
  60. lora_unet_double_blocks_11_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
  61. lora_unet_double_blocks_12_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
  62. lora_unet_double_blocks_12_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
  63. lora_unet_double_blocks_12_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
  64. lora_unet_double_blocks_12_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
  65. lora_unet_double_blocks_12_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
  66. lora_unet_double_blocks_12_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
  67. lora_unet_double_blocks_12_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
  68. lora_unet_double_blocks_12_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
  69. lora_unet_double_blocks_12_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
  70. lora_unet_double_blocks_12_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
  71. lora_unet_double_blocks_12_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
  72. lora_unet_double_blocks_12_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
  73. lora_unet_double_blocks_12_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
  74. lora_unet_double_blocks_12_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
  75. lora_unet_double_blocks_12_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
  76. lora_unet_double_blocks_12_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
  77. lora_unet_double_blocks_12_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
  78. lora_unet_double_blocks_12_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
  79. lora_unet_double_blocks_12_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
  80. lora_unet_double_blocks_12_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
  81. lora_unet_double_blocks_13_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
  82. lora_unet_double_blocks_13_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
  83. lora_unet_double_blocks_13_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
  84. lora_unet_double_blocks_13_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
  85. lora_unet_double_blocks_13_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
  86. lora_unet_double_blocks_13_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
  87. lora_unet_double_blocks_13_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
  88. lora_unet_double_blocks_13_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
  89. lora_unet_double_blocks_13_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
  90. lora_unet_double_blocks_13_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
  91. lora_unet_double_blocks_13_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
  92. lora_unet_double_blocks_13_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
  93. lora_unet_double_blocks_13_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
  94. lora_unet_double_blocks_13_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
  95. lora_unet_double_blocks_13_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
  96. lora_unet_double_blocks_13_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
  97. lora_unet_double_blocks_13_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
  98. lora_unet_double_blocks_13_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
  99. lora_unet_double_blocks_13_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 100. lora_unet_double_blocks_13_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 101. lora_unet_double_blocks_14_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 102. lora_unet_double_blocks_14_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 103. lora_unet_double_blocks_14_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 104. lora_unet_double_blocks_14_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 105. lora_unet_double_blocks_14_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 106. lora_unet_double_blocks_14_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 107. lora_unet_double_blocks_14_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 108. lora_unet_double_blocks_14_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 109. lora_unet_double_blocks_14_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 110. lora_unet_double_blocks_14_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 111. lora_unet_double_blocks_14_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 112. lora_unet_double_blocks_14_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 113. lora_unet_double_blocks_14_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 114. lora_unet_double_blocks_14_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 115. lora_unet_double_blocks_14_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 116. lora_unet_double_blocks_14_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 117. lora_unet_double_blocks_14_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 118. lora_unet_double_blocks_14_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 119. lora_unet_double_blocks_14_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 120. lora_unet_double_blocks_14_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 121. lora_unet_double_blocks_15_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 122. lora_unet_double_blocks_15_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 123. lora_unet_double_blocks_15_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 124. lora_unet_double_blocks_15_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 125. lora_unet_double_blocks_15_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 126. lora_unet_double_blocks_15_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 127. lora_unet_double_blocks_15_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 128. lora_unet_double_blocks_15_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 129. lora_unet_double_blocks_15_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 130. lora_unet_double_blocks_15_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 131. lora_unet_double_blocks_15_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 132. lora_unet_double_blocks_15_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 133. lora_unet_double_blocks_15_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 134. lora_unet_double_blocks_15_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 135. lora_unet_double_blocks_15_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 136. lora_unet_double_blocks_15_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 137. lora_unet_double_blocks_15_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 138. lora_unet_double_blocks_15_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 139. lora_unet_double_blocks_15_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 140. lora_unet_double_blocks_15_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 141. lora_unet_double_blocks_16_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 142. lora_unet_double_blocks_16_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 143. lora_unet_double_blocks_16_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 144. lora_unet_double_blocks_16_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 145. lora_unet_double_blocks_16_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 146. lora_unet_double_blocks_16_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 147. lora_unet_double_blocks_16_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 148. lora_unet_double_blocks_16_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 149. lora_unet_double_blocks_16_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 150. lora_unet_double_blocks_16_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 151. lora_unet_double_blocks_16_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 152. lora_unet_double_blocks_16_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 153. lora_unet_double_blocks_16_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 154. lora_unet_double_blocks_16_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 155. lora_unet_double_blocks_16_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 156. lora_unet_double_blocks_16_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 157. lora_unet_double_blocks_16_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 158. lora_unet_double_blocks_16_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 159. lora_unet_double_blocks_16_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 160. lora_unet_double_blocks_16_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 161. lora_unet_double_blocks_17_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 162. lora_unet_double_blocks_17_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 163. lora_unet_double_blocks_17_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 164. lora_unet_double_blocks_17_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 165. lora_unet_double_blocks_17_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 166. lora_unet_double_blocks_17_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 167. lora_unet_double_blocks_17_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 168. lora_unet_double_blocks_17_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 169. lora_unet_double_blocks_17_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 170. lora_unet_double_blocks_17_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 171. lora_unet_double_blocks_17_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 172. lora_unet_double_blocks_17_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 173. lora_unet_double_blocks_17_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 174. lora_unet_double_blocks_17_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 175. lora_unet_double_blocks_17_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 176. lora_unet_double_blocks_17_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 177. lora_unet_double_blocks_17_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 178. lora_unet_double_blocks_17_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 179. lora_unet_double_blocks_17_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 180. lora_unet_double_blocks_17_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 181. lora_unet_double_blocks_18_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 182. lora_unet_double_blocks_18_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 183. lora_unet_double_blocks_18_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 184. lora_unet_double_blocks_18_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 185. lora_unet_double_blocks_18_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 186. lora_unet_double_blocks_18_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 187. lora_unet_double_blocks_18_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 188. lora_unet_double_blocks_18_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 189. lora_unet_double_blocks_18_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 190. lora_unet_double_blocks_18_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 191. lora_unet_double_blocks_18_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 192. lora_unet_double_blocks_18_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 193. lora_unet_double_blocks_18_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 194. lora_unet_double_blocks_18_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 195. lora_unet_double_blocks_18_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 196. lora_unet_double_blocks_18_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 197. lora_unet_double_blocks_18_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 198. lora_unet_double_blocks_18_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 199. lora_unet_double_blocks_18_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 200. lora_unet_double_blocks_18_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 201. lora_unet_double_blocks_1_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 202. lora_unet_double_blocks_1_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 203. lora_unet_double_blocks_1_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 204. lora_unet_double_blocks_1_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 205. lora_unet_double_blocks_1_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 206. lora_unet_double_blocks_1_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 207. lora_unet_double_blocks_1_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 208. lora_unet_double_blocks_1_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 209. lora_unet_double_blocks_1_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 210. lora_unet_double_blocks_1_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 211. lora_unet_double_blocks_1_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 212. lora_unet_double_blocks_1_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 213. lora_unet_double_blocks_1_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 214. lora_unet_double_blocks_1_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 215. lora_unet_double_blocks_1_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 216. lora_unet_double_blocks_1_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 217. lora_unet_double_blocks_1_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 218. lora_unet_double_blocks_1_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 219. lora_unet_double_blocks_1_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 220. lora_unet_double_blocks_1_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 221. lora_unet_double_blocks_2_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 222. lora_unet_double_blocks_2_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 223. lora_unet_double_blocks_2_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 224. lora_unet_double_blocks_2_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 225. lora_unet_double_blocks_2_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 226. lora_unet_double_blocks_2_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 227. lora_unet_double_blocks_2_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 228. lora_unet_double_blocks_2_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 229. lora_unet_double_blocks_2_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 230. lora_unet_double_blocks_2_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 231. lora_unet_double_blocks_2_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 232. lora_unet_double_blocks_2_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 233. lora_unet_double_blocks_2_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 234. lora_unet_double_blocks_2_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 235. lora_unet_double_blocks_2_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 236. lora_unet_double_blocks_2_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 237. lora_unet_double_blocks_2_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 238. lora_unet_double_blocks_2_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 239. lora_unet_double_blocks_2_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 240. lora_unet_double_blocks_2_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 241. lora_unet_double_blocks_3_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 242. lora_unet_double_blocks_3_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 243. lora_unet_double_blocks_3_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 244. lora_unet_double_blocks_3_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 245. lora_unet_double_blocks_3_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 246. lora_unet_double_blocks_3_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 247. lora_unet_double_blocks_3_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 248. lora_unet_double_blocks_3_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 249. lora_unet_double_blocks_3_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 250. lora_unet_double_blocks_3_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 251. lora_unet_double_blocks_3_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 252. lora_unet_double_blocks_3_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 253. lora_unet_double_blocks_3_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 254. lora_unet_double_blocks_3_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 255. lora_unet_double_blocks_3_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 256. lora_unet_double_blocks_3_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 257. lora_unet_double_blocks_3_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 258. lora_unet_double_blocks_3_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 259. lora_unet_double_blocks_3_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 260. lora_unet_double_blocks_3_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 261. lora_unet_double_blocks_4_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 262. lora_unet_double_blocks_4_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 263. lora_unet_double_blocks_4_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 264. lora_unet_double_blocks_4_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 265. lora_unet_double_blocks_4_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 266. lora_unet_double_blocks_4_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 267. lora_unet_double_blocks_4_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 268. lora_unet_double_blocks_4_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 269. lora_unet_double_blocks_4_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 270. lora_unet_double_blocks_4_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 271. lora_unet_double_blocks_4_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 272. lora_unet_double_blocks_4_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 273. lora_unet_double_blocks_4_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 274. lora_unet_double_blocks_4_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 275. lora_unet_double_blocks_4_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 276. lora_unet_double_blocks_4_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 277. lora_unet_double_blocks_4_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 278. lora_unet_double_blocks_4_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 279. lora_unet_double_blocks_4_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 280. lora_unet_double_blocks_4_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 281. lora_unet_double_blocks_5_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 282. lora_unet_double_blocks_5_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 283. lora_unet_double_blocks_5_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 284. lora_unet_double_blocks_5_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 285. lora_unet_double_blocks_5_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 286. lora_unet_double_blocks_5_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 287. lora_unet_double_blocks_5_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 288. lora_unet_double_blocks_5_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 289. lora_unet_double_blocks_5_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 290. lora_unet_double_blocks_5_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 291. lora_unet_double_blocks_5_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 292. lora_unet_double_blocks_5_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 293. lora_unet_double_blocks_5_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 294. lora_unet_double_blocks_5_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 295. lora_unet_double_blocks_5_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 296. lora_unet_double_blocks_5_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 297. lora_unet_double_blocks_5_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 298. lora_unet_double_blocks_5_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 299. lora_unet_double_blocks_5_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 300. lora_unet_double_blocks_5_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 301. lora_unet_double_blocks_6_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 302. lora_unet_double_blocks_6_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 303. lora_unet_double_blocks_6_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 304. lora_unet_double_blocks_6_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 305. lora_unet_double_blocks_6_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 306. lora_unet_double_blocks_6_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 307. lora_unet_double_blocks_6_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 308. lora_unet_double_blocks_6_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 309. lora_unet_double_blocks_6_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 310. lora_unet_double_blocks_6_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 311. lora_unet_double_blocks_6_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 312. lora_unet_double_blocks_6_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 313. lora_unet_double_blocks_6_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 314. lora_unet_double_blocks_6_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 315. lora_unet_double_blocks_6_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 316. lora_unet_double_blocks_6_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 317. lora_unet_double_blocks_6_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 318. lora_unet_double_blocks_6_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 319. lora_unet_double_blocks_6_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 320. lora_unet_double_blocks_6_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 321. lora_unet_double_blocks_7_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 322. lora_unet_double_blocks_7_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 323. lora_unet_double_blocks_7_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 324. lora_unet_double_blocks_7_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 325. lora_unet_double_blocks_7_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 326. lora_unet_double_blocks_7_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 327. lora_unet_double_blocks_7_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 328. lora_unet_double_blocks_7_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 329. lora_unet_double_blocks_7_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 330. lora_unet_double_blocks_7_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 331. lora_unet_double_blocks_7_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 332. lora_unet_double_blocks_7_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 333. lora_unet_double_blocks_7_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 334. lora_unet_double_blocks_7_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 335. lora_unet_double_blocks_7_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 336. lora_unet_double_blocks_7_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 337. lora_unet_double_blocks_7_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 338. lora_unet_double_blocks_7_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 339. lora_unet_double_blocks_7_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 340. lora_unet_double_blocks_7_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 341. lora_unet_double_blocks_8_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 342. lora_unet_double_blocks_8_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 343. lora_unet_double_blocks_8_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 344. lora_unet_double_blocks_8_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 345. lora_unet_double_blocks_8_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 346. lora_unet_double_blocks_8_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 347. lora_unet_double_blocks_8_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 348. lora_unet_double_blocks_8_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 349. lora_unet_double_blocks_8_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 350. lora_unet_double_blocks_8_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 351. lora_unet_double_blocks_8_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 352. lora_unet_double_blocks_8_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 353. lora_unet_double_blocks_8_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 354. lora_unet_double_blocks_8_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 355. lora_unet_double_blocks_8_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 356. lora_unet_double_blocks_8_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 357. lora_unet_double_blocks_8_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 358. lora_unet_double_blocks_8_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 359. lora_unet_double_blocks_8_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 360. lora_unet_double_blocks_8_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 361. lora_unet_double_blocks_9_img_attn_proj.lora_down.weight: torch.Size([4, 3072])
 362. lora_unet_double_blocks_9_img_attn_proj.lora_up.weight: torch.Size([3072, 4])
 363. lora_unet_double_blocks_9_img_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 364. lora_unet_double_blocks_9_img_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 365. lora_unet_double_blocks_9_img_mlp_0.lora_down.weight: torch.Size([4, 3072])
 366. lora_unet_double_blocks_9_img_mlp_0.lora_up.weight: torch.Size([12288, 4])
 367. lora_unet_double_blocks_9_img_mlp_2.lora_down.weight: torch.Size([4, 12288])
 368. lora_unet_double_blocks_9_img_mlp_2.lora_up.weight: torch.Size([3072, 4])
 369. lora_unet_double_blocks_9_img_mod_lin.lora_down.weight: torch.Size([4, 3072])
 370. lora_unet_double_blocks_9_img_mod_lin.lora_up.weight: torch.Size([18432, 4])
 371. lora_unet_double_blocks_9_txt_attn_proj.lora_down.weight: torch.Size([4, 3072])
 372. lora_unet_double_blocks_9_txt_attn_proj.lora_up.weight: torch.Size([3072, 4])
 373. lora_unet_double_blocks_9_txt_attn_qkv.lora_down.weight: torch.Size([4, 3072])
 374. lora_unet_double_blocks_9_txt_attn_qkv.lora_up.weight: torch.Size([9216, 4])
 375. lora_unet_double_blocks_9_txt_mlp_0.lora_down.weight: torch.Size([4, 3072])
 376. lora_unet_double_blocks_9_txt_mlp_0.lora_up.weight: torch.Size([12288, 4])
 377. lora_unet_double_blocks_9_txt_mlp_2.lora_down.weight: torch.Size([4, 12288])
 378. lora_unet_double_blocks_9_txt_mlp_2.lora_up.weight: torch.Size([3072, 4])
 379. lora_unet_double_blocks_9_txt_mod_lin.lora_down.weight: torch.Size([4, 3072])
 380. lora_unet_double_blocks_9_txt_mod_lin.lora_up.weight: torch.Size([18432, 4])
 381. lora_unet_single_blocks_0_linear1.lora_down.weight: torch.Size([4, 3072])
 382. lora_unet_single_blocks_0_linear1.lora_up.weight: torch.Size([21504, 4])
 383. lora_unet_single_blocks_0_linear2.lora_down.weight: torch.Size([4, 15360])
 384. lora_unet_single_blocks_0_linear2.lora_up.weight: torch.Size([3072, 4])
 385. lora_unet_single_blocks_0_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 386. lora_unet_single_blocks_0_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 387. lora_unet_single_blocks_10_linear1.lora_down.weight: torch.Size([4, 3072])
 388. lora_unet_single_blocks_10_linear1.lora_up.weight: torch.Size([21504, 4])
 389. lora_unet_single_blocks_10_linear2.lora_down.weight: torch.Size([4, 15360])
 390. lora_unet_single_blocks_10_linear2.lora_up.weight: torch.Size([3072, 4])
 391. lora_unet_single_blocks_10_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 392. lora_unet_single_blocks_10_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 393. lora_unet_single_blocks_11_linear1.lora_down.weight: torch.Size([4, 3072])
 394. lora_unet_single_blocks_11_linear1.lora_up.weight: torch.Size([21504, 4])
 395. lora_unet_single_blocks_11_linear2.lora_down.weight: torch.Size([4, 15360])
 396. lora_unet_single_blocks_11_linear2.lora_up.weight: torch.Size([3072, 4])
 397. lora_unet_single_blocks_11_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 398. lora_unet_single_blocks_11_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 399. lora_unet_single_blocks_12_linear1.lora_down.weight: torch.Size([4, 3072])
 400. lora_unet_single_blocks_12_linear1.lora_up.weight: torch.Size([21504, 4])
 401. lora_unet_single_blocks_12_linear2.lora_down.weight: torch.Size([4, 15360])
 402. lora_unet_single_blocks_12_linear2.lora_up.weight: torch.Size([3072, 4])
 403. lora_unet_single_blocks_12_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 404. lora_unet_single_blocks_12_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 405. lora_unet_single_blocks_13_linear1.lora_down.weight: torch.Size([4, 3072])
 406. lora_unet_single_blocks_13_linear1.lora_up.weight: torch.Size([21504, 4])
 407. lora_unet_single_blocks_13_linear2.lora_down.weight: torch.Size([4, 15360])
 408. lora_unet_single_blocks_13_linear2.lora_up.weight: torch.Size([3072, 4])
 409. lora_unet_single_blocks_13_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 410. lora_unet_single_blocks_13_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 411. lora_unet_single_blocks_14_linear1.lora_down.weight: torch.Size([4, 3072])
 412. lora_unet_single_blocks_14_linear1.lora_up.weight: torch.Size([21504, 4])
 413. lora_unet_single_blocks_14_linear2.lora_down.weight: torch.Size([4, 15360])
 414. lora_unet_single_blocks_14_linear2.lora_up.weight: torch.Size([3072, 4])
 415. lora_unet_single_blocks_14_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 416. lora_unet_single_blocks_14_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 417. lora_unet_single_blocks_15_linear1.lora_down.weight: torch.Size([4, 3072])
 418. lora_unet_single_blocks_15_linear1.lora_up.weight: torch.Size([21504, 4])
 419. lora_unet_single_blocks_15_linear2.lora_down.weight: torch.Size([4, 15360])
 420. lora_unet_single_blocks_15_linear2.lora_up.weight: torch.Size([3072, 4])
 421. lora_unet_single_blocks_15_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 422. lora_unet_single_blocks_15_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 423. lora_unet_single_blocks_16_linear1.lora_down.weight: torch.Size([4, 3072])
 424. lora_unet_single_blocks_16_linear1.lora_up.weight: torch.Size([21504, 4])
 425. lora_unet_single_blocks_16_linear2.lora_down.weight: torch.Size([4, 15360])
 426. lora_unet_single_blocks_16_linear2.lora_up.weight: torch.Size([3072, 4])
 427. lora_unet_single_blocks_16_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 428. lora_unet_single_blocks_16_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 429. lora_unet_single_blocks_17_linear1.lora_down.weight: torch.Size([4, 3072])
 430. lora_unet_single_blocks_17_linear1.lora_up.weight: torch.Size([21504, 4])
 431. lora_unet_single_blocks_17_linear2.lora_down.weight: torch.Size([4, 15360])
 432. lora_unet_single_blocks_17_linear2.lora_up.weight: torch.Size([3072, 4])
 433. lora_unet_single_blocks_17_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 434. lora_unet_single_blocks_17_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 435. lora_unet_single_blocks_18_linear1.lora_down.weight: torch.Size([4, 3072])
 436. lora_unet_single_blocks_18_linear1.lora_up.weight: torch.Size([21504, 4])
 437. lora_unet_single_blocks_18_linear2.lora_down.weight: torch.Size([4, 15360])
 438. lora_unet_single_blocks_18_linear2.lora_up.weight: torch.Size([3072, 4])
 439. lora_unet_single_blocks_18_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 440. lora_unet_single_blocks_18_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 441. lora_unet_single_blocks_19_linear1.lora_down.weight: torch.Size([4, 3072])
 442. lora_unet_single_blocks_19_linear1.lora_up.weight: torch.Size([21504, 4])
 443. lora_unet_single_blocks_19_linear2.lora_down.weight: torch.Size([4, 15360])
 444. lora_unet_single_blocks_19_linear2.lora_up.weight: torch.Size([3072, 4])
 445. lora_unet_single_blocks_19_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 446. lora_unet_single_blocks_19_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 447. lora_unet_single_blocks_1_linear1.lora_down.weight: torch.Size([4, 3072])
 448. lora_unet_single_blocks_1_linear1.lora_up.weight: torch.Size([21504, 4])
 449. lora_unet_single_blocks_1_linear2.lora_down.weight: torch.Size([4, 15360])
 450. lora_unet_single_blocks_1_linear2.lora_up.weight: torch.Size([3072, 4])
 451. lora_unet_single_blocks_1_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 452. lora_unet_single_blocks_1_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 453. lora_unet_single_blocks_20_linear1.lora_down.weight: torch.Size([4, 3072])
 454. lora_unet_single_blocks_20_linear1.lora_up.weight: torch.Size([21504, 4])
 455. lora_unet_single_blocks_20_linear2.lora_down.weight: torch.Size([4, 15360])
 456. lora_unet_single_blocks_20_linear2.lora_up.weight: torch.Size([3072, 4])
 457. lora_unet_single_blocks_20_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 458. lora_unet_single_blocks_20_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 459. lora_unet_single_blocks_21_linear1.lora_down.weight: torch.Size([4, 3072])
 460. lora_unet_single_blocks_21_linear1.lora_up.weight: torch.Size([21504, 4])
 461. lora_unet_single_blocks_21_linear2.lora_down.weight: torch.Size([4, 15360])
 462. lora_unet_single_blocks_21_linear2.lora_up.weight: torch.Size([3072, 4])
 463. lora_unet_single_blocks_21_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 464. lora_unet_single_blocks_21_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 465. lora_unet_single_blocks_22_linear1.lora_down.weight: torch.Size([4, 3072])
 466. lora_unet_single_blocks_22_linear1.lora_up.weight: torch.Size([21504, 4])
 467. lora_unet_single_blocks_22_linear2.lora_down.weight: torch.Size([4, 15360])
 468. lora_unet_single_blocks_22_linear2.lora_up.weight: torch.Size([3072, 4])
 469. lora_unet_single_blocks_22_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 470. lora_unet_single_blocks_22_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 471. lora_unet_single_blocks_23_linear1.lora_down.weight: torch.Size([4, 3072])
 472. lora_unet_single_blocks_23_linear1.lora_up.weight: torch.Size([21504, 4])
 473. lora_unet_single_blocks_23_linear2.lora_down.weight: torch.Size([4, 15360])
 474. lora_unet_single_blocks_23_linear2.lora_up.weight: torch.Size([3072, 4])
 475. lora_unet_single_blocks_23_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 476. lora_unet_single_blocks_23_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 477. lora_unet_single_blocks_24_linear1.lora_down.weight: torch.Size([4, 3072])
 478. lora_unet_single_blocks_24_linear1.lora_up.weight: torch.Size([21504, 4])
 479. lora_unet_single_blocks_24_linear2.lora_down.weight: torch.Size([4, 15360])
 480. lora_unet_single_blocks_24_linear2.lora_up.weight: torch.Size([3072, 4])
 481. lora_unet_single_blocks_24_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 482. lora_unet_single_blocks_24_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 483. lora_unet_single_blocks_25_linear1.lora_down.weight: torch.Size([4, 3072])
 484. lora_unet_single_blocks_25_linear1.lora_up.weight: torch.Size([21504, 4])
 485. lora_unet_single_blocks_25_linear2.lora_down.weight: torch.Size([4, 15360])
 486. lora_unet_single_blocks_25_linear2.lora_up.weight: torch.Size([3072, 4])
 487. lora_unet_single_blocks_25_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 488. lora_unet_single_blocks_25_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 489. lora_unet_single_blocks_26_linear1.lora_down.weight: torch.Size([4, 3072])
 490. lora_unet_single_blocks_26_linear1.lora_up.weight: torch.Size([21504, 4])
 491. lora_unet_single_blocks_26_linear2.lora_down.weight: torch.Size([4, 15360])
 492. lora_unet_single_blocks_26_linear2.lora_up.weight: torch.Size([3072, 4])
 493. lora_unet_single_blocks_26_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 494. lora_unet_single_blocks_26_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 495. lora_unet_single_blocks_27_linear1.lora_down.weight: torch.Size([4, 3072])
 496. lora_unet_single_blocks_27_linear1.lora_up.weight: torch.Size([21504, 4])
 497. lora_unet_single_blocks_27_linear2.lora_down.weight: torch.Size([4, 15360])
 498. lora_unet_single_blocks_27_linear2.lora_up.weight: torch.Size([3072, 4])
 499. lora_unet_single_blocks_27_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 500. lora_unet_single_blocks_27_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 501. lora_unet_single_blocks_28_linear1.lora_down.weight: torch.Size([4, 3072])
 502. lora_unet_single_blocks_28_linear1.lora_up.weight: torch.Size([21504, 4])
 503. lora_unet_single_blocks_28_linear2.lora_down.weight: torch.Size([4, 15360])
 504. lora_unet_single_blocks_28_linear2.lora_up.weight: torch.Size([3072, 4])
 505. lora_unet_single_blocks_28_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 506. lora_unet_single_blocks_28_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 507. lora_unet_single_blocks_29_linear1.lora_down.weight: torch.Size([4, 3072])
 508. lora_unet_single_blocks_29_linear1.lora_up.weight: torch.Size([21504, 4])
 509. lora_unet_single_blocks_29_linear2.lora_down.weight: torch.Size([4, 15360])
 510. lora_unet_single_blocks_29_linear2.lora_up.weight: torch.Size([3072, 4])
 511. lora_unet_single_blocks_29_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 512. lora_unet_single_blocks_29_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 513. lora_unet_single_blocks_2_linear1.lora_down.weight: torch.Size([4, 3072])
 514. lora_unet_single_blocks_2_linear1.lora_up.weight: torch.Size([21504, 4])
 515. lora_unet_single_blocks_2_linear2.lora_down.weight: torch.Size([4, 15360])
 516. lora_unet_single_blocks_2_linear2.lora_up.weight: torch.Size([3072, 4])
 517. lora_unet_single_blocks_2_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 518. lora_unet_single_blocks_2_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 519. lora_unet_single_blocks_30_linear1.lora_down.weight: torch.Size([4, 3072])
 520. lora_unet_single_blocks_30_linear1.lora_up.weight: torch.Size([21504, 4])
 521. lora_unet_single_blocks_30_linear2.lora_down.weight: torch.Size([4, 15360])
 522. lora_unet_single_blocks_30_linear2.lora_up.weight: torch.Size([3072, 4])
 523. lora_unet_single_blocks_30_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 524. lora_unet_single_blocks_30_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 525. lora_unet_single_blocks_31_linear1.lora_down.weight: torch.Size([4, 3072])
 526. lora_unet_single_blocks_31_linear1.lora_up.weight: torch.Size([21504, 4])
 527. lora_unet_single_blocks_31_linear2.lora_down.weight: torch.Size([4, 15360])
 528. lora_unet_single_blocks_31_linear2.lora_up.weight: torch.Size([3072, 4])
 529. lora_unet_single_blocks_31_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 530. lora_unet_single_blocks_31_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 531. lora_unet_single_blocks_32_linear1.lora_down.weight: torch.Size([4, 3072])
 532. lora_unet_single_blocks_32_linear1.lora_up.weight: torch.Size([21504, 4])
 533. lora_unet_single_blocks_32_linear2.lora_down.weight: torch.Size([4, 15360])
 534. lora_unet_single_blocks_32_linear2.lora_up.weight: torch.Size([3072, 4])
 535. lora_unet_single_blocks_32_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 536. lora_unet_single_blocks_32_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 537. lora_unet_single_blocks_33_linear1.lora_down.weight: torch.Size([4, 3072])
 538. lora_unet_single_blocks_33_linear1.lora_up.weight: torch.Size([21504, 4])
 539. lora_unet_single_blocks_33_linear2.lora_down.weight: torch.Size([4, 15360])
 540. lora_unet_single_blocks_33_linear2.lora_up.weight: torch.Size([3072, 4])
 541. lora_unet_single_blocks_33_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 542. lora_unet_single_blocks_33_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 543. lora_unet_single_blocks_34_linear1.lora_down.weight: torch.Size([4, 3072])
 544. lora_unet_single_blocks_34_linear1.lora_up.weight: torch.Size([21504, 4])
 545. lora_unet_single_blocks_34_linear2.lora_down.weight: torch.Size([4, 15360])
 546. lora_unet_single_blocks_34_linear2.lora_up.weight: torch.Size([3072, 4])
 547. lora_unet_single_blocks_34_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 548. lora_unet_single_blocks_34_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 549. lora_unet_single_blocks_35_linear1.lora_down.weight: torch.Size([4, 3072])
 550. lora_unet_single_blocks_35_linear1.lora_up.weight: torch.Size([21504, 4])
 551. lora_unet_single_blocks_35_linear2.lora_down.weight: torch.Size([4, 15360])
 552. lora_unet_single_blocks_35_linear2.lora_up.weight: torch.Size([3072, 4])
 553. lora_unet_single_blocks_35_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 554. lora_unet_single_blocks_35_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 555. lora_unet_single_blocks_36_linear1.lora_down.weight: torch.Size([4, 3072])
 556. lora_unet_single_blocks_36_linear1.lora_up.weight: torch.Size([21504, 4])
 557. lora_unet_single_blocks_36_linear2.lora_down.weight: torch.Size([4, 15360])
 558. lora_unet_single_blocks_36_linear2.lora_up.weight: torch.Size([3072, 4])
 559. lora_unet_single_blocks_36_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 560. lora_unet_single_blocks_36_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 561. lora_unet_single_blocks_37_linear1.lora_down.weight: torch.Size([4, 3072])
 562. lora_unet_single_blocks_37_linear1.lora_up.weight: torch.Size([21504, 4])
 563. lora_unet_single_blocks_37_linear2.lora_down.weight: torch.Size([4, 15360])
 564. lora_unet_single_blocks_37_linear2.lora_up.weight: torch.Size([3072, 4])
 565. lora_unet_single_blocks_37_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 566. lora_unet_single_blocks_37_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 567. lora_unet_single_blocks_3_linear1.lora_down.weight: torch.Size([4, 3072])
 568. lora_unet_single_blocks_3_linear1.lora_up.weight: torch.Size([21504, 4])
 569. lora_unet_single_blocks_3_linear2.lora_down.weight: torch.Size([4, 15360])
 570. lora_unet_single_blocks_3_linear2.lora_up.weight: torch.Size([3072, 4])
 571. lora_unet_single_blocks_3_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 572. lora_unet_single_blocks_3_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 573. lora_unet_single_blocks_4_linear1.lora_down.weight: torch.Size([4, 3072])
 574. lora_unet_single_blocks_4_linear1.lora_up.weight: torch.Size([21504, 4])
 575. lora_unet_single_blocks_4_linear2.lora_down.weight: torch.Size([4, 15360])
 576. lora_unet_single_blocks_4_linear2.lora_up.weight: torch.Size([3072, 4])
 577. lora_unet_single_blocks_4_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 578. lora_unet_single_blocks_4_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 579. lora_unet_single_blocks_5_linear1.lora_down.weight: torch.Size([4, 3072])
 580. lora_unet_single_blocks_5_linear1.lora_up.weight: torch.Size([21504, 4])
 581. lora_unet_single_blocks_5_linear2.lora_down.weight: torch.Size([4, 15360])
 582. lora_unet_single_blocks_5_linear2.lora_up.weight: torch.Size([3072, 4])
 583. lora_unet_single_blocks_5_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 584. lora_unet_single_blocks_5_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 585. lora_unet_single_blocks_6_linear1.lora_down.weight: torch.Size([4, 3072])
 586. lora_unet_single_blocks_6_linear1.lora_up.weight: torch.Size([21504, 4])
 587. lora_unet_single_blocks_6_linear2.lora_down.weight: torch.Size([4, 15360])
 588. lora_unet_single_blocks_6_linear2.lora_up.weight: torch.Size([3072, 4])
 589. lora_unet_single_blocks_6_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 590. lora_unet_single_blocks_6_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 591. lora_unet_single_blocks_7_linear1.lora_down.weight: torch.Size([4, 3072])
 592. lora_unet_single_blocks_7_linear1.lora_up.weight: torch.Size([21504, 4])
 593. lora_unet_single_blocks_7_linear2.lora_down.weight: torch.Size([4, 15360])
 594. lora_unet_single_blocks_7_linear2.lora_up.weight: torch.Size([3072, 4])
 595. lora_unet_single_blocks_7_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 596. lora_unet_single_blocks_7_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 597. lora_unet_single_blocks_8_linear1.lora_down.weight: torch.Size([4, 3072])
 598. lora_unet_single_blocks_8_linear1.lora_up.weight: torch.Size([21504, 4])
 599. lora_unet_single_blocks_8_linear2.lora_down.weight: torch.Size([4, 15360])
 600. lora_unet_single_blocks_8_linear2.lora_up.weight: torch.Size([3072, 4])
 601. lora_unet_single_blocks_8_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 602. lora_unet_single_blocks_8_modulation_lin.lora_up.weight: torch.Size([9216, 4])
 603. lora_unet_single_blocks_9_linear1.lora_down.weight: torch.Size([4, 3072])
 604. lora_unet_single_blocks_9_linear1.lora_up.weight: torch.Size([21504, 4])
 605. lora_unet_single_blocks_9_linear2.lora_down.weight: torch.Size([4, 15360])
 606. lora_unet_single_blocks_9_linear2.lora_up.weight: torch.Size([3072, 4])
 607. lora_unet_single_blocks_9_modulation_lin.lora_down.weight: torch.Size([4, 3072])
 608. lora_unet_single_blocks_9_modulation_lin.lora_up.weight: torch.Size([9216, 4])

Found 304 alpha parameters:
   1. lora_unet_double_blocks_0_img_attn_proj.alpha: 1.0
   2. lora_unet_double_blocks_0_img_attn_qkv.alpha: 1.0
   3. lora_unet_double_blocks_0_img_mlp_0.alpha: 1.0
   4. lora_unet_double_blocks_0_img_mlp_2.alpha: 1.0
   5. lora_unet_double_blocks_0_img_mod_lin.alpha: 1.0
   6. lora_unet_double_blocks_0_txt_attn_proj.alpha: 1.0
   7. lora_unet_double_blocks_0_txt_attn_qkv.alpha: 1.0
   8. lora_unet_double_blocks_0_txt_mlp_0.alpha: 1.0
   9. lora_unet_double_blocks_0_txt_mlp_2.alpha: 1.0
  10. lora_unet_double_blocks_0_txt_mod_lin.alpha: 1.0
  11. lora_unet_double_blocks_10_img_attn_proj.alpha: 1.0
  12. lora_unet_double_blocks_10_img_attn_qkv.alpha: 1.0
  13. lora_unet_double_blocks_10_img_mlp_0.alpha: 1.0
  14. lora_unet_double_blocks_10_img_mlp_2.alpha: 1.0
  15. lora_unet_double_blocks_10_img_mod_lin.alpha: 1.0
  16. lora_unet_double_blocks_10_txt_attn_proj.alpha: 1.0
  17. lora_unet_double_blocks_10_txt_attn_qkv.alpha: 1.0
  18. lora_unet_double_blocks_10_txt_mlp_0.alpha: 1.0
  19. lora_unet_double_blocks_10_txt_mlp_2.alpha: 1.0
  20. lora_unet_double_blocks_10_txt_mod_lin.alpha: 1.0
  21. lora_unet_double_blocks_11_img_attn_proj.alpha: 1.0
  22. lora_unet_double_blocks_11_img_attn_qkv.alpha: 1.0
  23. lora_unet_double_blocks_11_img_mlp_0.alpha: 1.0
  24. lora_unet_double_blocks_11_img_mlp_2.alpha: 1.0
  25. lora_unet_double_blocks_11_img_mod_lin.alpha: 1.0
  26. lora_unet_double_blocks_11_txt_attn_proj.alpha: 1.0
  27. lora_unet_double_blocks_11_txt_attn_qkv.alpha: 1.0
  28. lora_unet_double_blocks_11_txt_mlp_0.alpha: 1.0
  29. lora_unet_double_blocks_11_txt_mlp_2.alpha: 1.0
  30. lora_unet_double_blocks_11_txt_mod_lin.alpha: 1.0
  31. lora_unet_double_blocks_12_img_attn_proj.alpha: 1.0
  32. lora_unet_double_blocks_12_img_attn_qkv.alpha: 1.0
  33. lora_unet_double_blocks_12_img_mlp_0.alpha: 1.0
  34. lora_unet_double_blocks_12_img_mlp_2.alpha: 1.0
  35. lora_unet_double_blocks_12_img_mod_lin.alpha: 1.0
  36. lora_unet_double_blocks_12_txt_attn_proj.alpha: 1.0
  37. lora_unet_double_blocks_12_txt_attn_qkv.alpha: 1.0
  38. lora_unet_double_blocks_12_txt_mlp_0.alpha: 1.0
  39. lora_unet_double_blocks_12_txt_mlp_2.alpha: 1.0
  40. lora_unet_double_blocks_12_txt_mod_lin.alpha: 1.0
  41. lora_unet_double_blocks_13_img_attn_proj.alpha: 1.0
  42. lora_unet_double_blocks_13_img_attn_qkv.alpha: 1.0
  43. lora_unet_double_blocks_13_img_mlp_0.alpha: 1.0
  44. lora_unet_double_blocks_13_img_mlp_2.alpha: 1.0
  45. lora_unet_double_blocks_13_img_mod_lin.alpha: 1.0
  46. lora_unet_double_blocks_13_txt_attn_proj.alpha: 1.0
  47. lora_unet_double_blocks_13_txt_attn_qkv.alpha: 1.0
  48. lora_unet_double_blocks_13_txt_mlp_0.alpha: 1.0
  49. lora_unet_double_blocks_13_txt_mlp_2.alpha: 1.0
  50. lora_unet_double_blocks_13_txt_mod_lin.alpha: 1.0
  51. lora_unet_double_blocks_14_img_attn_proj.alpha: 1.0
  52. lora_unet_double_blocks_14_img_attn_qkv.alpha: 1.0
  53. lora_unet_double_blocks_14_img_mlp_0.alpha: 1.0
  54. lora_unet_double_blocks_14_img_mlp_2.alpha: 1.0
  55. lora_unet_double_blocks_14_img_mod_lin.alpha: 1.0
  56. lora_unet_double_blocks_14_txt_attn_proj.alpha: 1.0
  57. lora_unet_double_blocks_14_txt_attn_qkv.alpha: 1.0
  58. lora_unet_double_blocks_14_txt_mlp_0.alpha: 1.0
  59. lora_unet_double_blocks_14_txt_mlp_2.alpha: 1.0
  60. lora_unet_double_blocks_14_txt_mod_lin.alpha: 1.0
  61. lora_unet_double_blocks_15_img_attn_proj.alpha: 1.0
  62. lora_unet_double_blocks_15_img_attn_qkv.alpha: 1.0
  63. lora_unet_double_blocks_15_img_mlp_0.alpha: 1.0
  64. lora_unet_double_blocks_15_img_mlp_2.alpha: 1.0
  65. lora_unet_double_blocks_15_img_mod_lin.alpha: 1.0
  66. lora_unet_double_blocks_15_txt_attn_proj.alpha: 1.0
  67. lora_unet_double_blocks_15_txt_attn_qkv.alpha: 1.0
  68. lora_unet_double_blocks_15_txt_mlp_0.alpha: 1.0
  69. lora_unet_double_blocks_15_txt_mlp_2.alpha: 1.0
  70. lora_unet_double_blocks_15_txt_mod_lin.alpha: 1.0
  71. lora_unet_double_blocks_16_img_attn_proj.alpha: 1.0
  72. lora_unet_double_blocks_16_img_attn_qkv.alpha: 1.0
  73. lora_unet_double_blocks_16_img_mlp_0.alpha: 1.0
  74. lora_unet_double_blocks_16_img_mlp_2.alpha: 1.0
  75. lora_unet_double_blocks_16_img_mod_lin.alpha: 1.0
  76. lora_unet_double_blocks_16_txt_attn_proj.alpha: 1.0
  77. lora_unet_double_blocks_16_txt_attn_qkv.alpha: 1.0
  78. lora_unet_double_blocks_16_txt_mlp_0.alpha: 1.0
  79. lora_unet_double_blocks_16_txt_mlp_2.alpha: 1.0
  80. lora_unet_double_blocks_16_txt_mod_lin.alpha: 1.0
  81. lora_unet_double_blocks_17_img_attn_proj.alpha: 1.0
  82. lora_unet_double_blocks_17_img_attn_qkv.alpha: 1.0
  83. lora_unet_double_blocks_17_img_mlp_0.alpha: 1.0
  84. lora_unet_double_blocks_17_img_mlp_2.alpha: 1.0
  85. lora_unet_double_blocks_17_img_mod_lin.alpha: 1.0
  86. lora_unet_double_blocks_17_txt_attn_proj.alpha: 1.0
  87. lora_unet_double_blocks_17_txt_attn_qkv.alpha: 1.0
  88. lora_unet_double_blocks_17_txt_mlp_0.alpha: 1.0
  89. lora_unet_double_blocks_17_txt_mlp_2.alpha: 1.0
  90. lora_unet_double_blocks_17_txt_mod_lin.alpha: 1.0
  91. lora_unet_double_blocks_18_img_attn_proj.alpha: 1.0
  92. lora_unet_double_blocks_18_img_attn_qkv.alpha: 1.0
  93. lora_unet_double_blocks_18_img_mlp_0.alpha: 1.0
  94. lora_unet_double_blocks_18_img_mlp_2.alpha: 1.0
  95. lora_unet_double_blocks_18_img_mod_lin.alpha: 1.0
  96. lora_unet_double_blocks_18_txt_attn_proj.alpha: 1.0
  97. lora_unet_double_blocks_18_txt_attn_qkv.alpha: 1.0
  98. lora_unet_double_blocks_18_txt_mlp_0.alpha: 1.0
  99. lora_unet_double_blocks_18_txt_mlp_2.alpha: 1.0
 100. lora_unet_double_blocks_18_txt_mod_lin.alpha: 1.0
 101. lora_unet_double_blocks_1_img_attn_proj.alpha: 1.0
 102. lora_unet_double_blocks_1_img_attn_qkv.alpha: 1.0
 103. lora_unet_double_blocks_1_img_mlp_0.alpha: 1.0
 104. lora_unet_double_blocks_1_img_mlp_2.alpha: 1.0
 105. lora_unet_double_blocks_1_img_mod_lin.alpha: 1.0
 106. lora_unet_double_blocks_1_txt_attn_proj.alpha: 1.0
 107. lora_unet_double_blocks_1_txt_attn_qkv.alpha: 1.0
 108. lora_unet_double_blocks_1_txt_mlp_0.alpha: 1.0
 109. lora_unet_double_blocks_1_txt_mlp_2.alpha: 1.0
 110. lora_unet_double_blocks_1_txt_mod_lin.alpha: 1.0
 111. lora_unet_double_blocks_2_img_attn_proj.alpha: 1.0
 112. lora_unet_double_blocks_2_img_attn_qkv.alpha: 1.0
 113. lora_unet_double_blocks_2_img_mlp_0.alpha: 1.0
 114. lora_unet_double_blocks_2_img_mlp_2.alpha: 1.0
 115. lora_unet_double_blocks_2_img_mod_lin.alpha: 1.0
 116. lora_unet_double_blocks_2_txt_attn_proj.alpha: 1.0
 117. lora_unet_double_blocks_2_txt_attn_qkv.alpha: 1.0
 118. lora_unet_double_blocks_2_txt_mlp_0.alpha: 1.0
 119. lora_unet_double_blocks_2_txt_mlp_2.alpha: 1.0
 120. lora_unet_double_blocks_2_txt_mod_lin.alpha: 1.0
 121. lora_unet_double_blocks_3_img_attn_proj.alpha: 1.0
 122. lora_unet_double_blocks_3_img_attn_qkv.alpha: 1.0
 123. lora_unet_double_blocks_3_img_mlp_0.alpha: 1.0
 124. lora_unet_double_blocks_3_img_mlp_2.alpha: 1.0
 125. lora_unet_double_blocks_3_img_mod_lin.alpha: 1.0
 126. lora_unet_double_blocks_3_txt_attn_proj.alpha: 1.0
 127. lora_unet_double_blocks_3_txt_attn_qkv.alpha: 1.0
 128. lora_unet_double_blocks_3_txt_mlp_0.alpha: 1.0
 129. lora_unet_double_blocks_3_txt_mlp_2.alpha: 1.0
 130. lora_unet_double_blocks_3_txt_mod_lin.alpha: 1.0
 131. lora_unet_double_blocks_4_img_attn_proj.alpha: 1.0
 132. lora_unet_double_blocks_4_img_attn_qkv.alpha: 1.0
 133. lora_unet_double_blocks_4_img_mlp_0.alpha: 1.0
 134. lora_unet_double_blocks_4_img_mlp_2.alpha: 1.0
 135. lora_unet_double_blocks_4_img_mod_lin.alpha: 1.0
 136. lora_unet_double_blocks_4_txt_attn_proj.alpha: 1.0
 137. lora_unet_double_blocks_4_txt_attn_qkv.alpha: 1.0
 138. lora_unet_double_blocks_4_txt_mlp_0.alpha: 1.0
 139. lora_unet_double_blocks_4_txt_mlp_2.alpha: 1.0
 140. lora_unet_double_blocks_4_txt_mod_lin.alpha: 1.0
 141. lora_unet_double_blocks_5_img_attn_proj.alpha: 1.0
 142. lora_unet_double_blocks_5_img_attn_qkv.alpha: 1.0
 143. lora_unet_double_blocks_5_img_mlp_0.alpha: 1.0
 144. lora_unet_double_blocks_5_img_mlp_2.alpha: 1.0
 145. lora_unet_double_blocks_5_img_mod_lin.alpha: 1.0
 146. lora_unet_double_blocks_5_txt_attn_proj.alpha: 1.0
 147. lora_unet_double_blocks_5_txt_attn_qkv.alpha: 1.0
 148. lora_unet_double_blocks_5_txt_mlp_0.alpha: 1.0
 149. lora_unet_double_blocks_5_txt_mlp_2.alpha: 1.0
 150. lora_unet_double_blocks_5_txt_mod_lin.alpha: 1.0
 151. lora_unet_double_blocks_6_img_attn_proj.alpha: 1.0
 152. lora_unet_double_blocks_6_img_attn_qkv.alpha: 1.0
 153. lora_unet_double_blocks_6_img_mlp_0.alpha: 1.0
 154. lora_unet_double_blocks_6_img_mlp_2.alpha: 1.0
 155. lora_unet_double_blocks_6_img_mod_lin.alpha: 1.0
 156. lora_unet_double_blocks_6_txt_attn_proj.alpha: 1.0
 157. lora_unet_double_blocks_6_txt_attn_qkv.alpha: 1.0
 158. lora_unet_double_blocks_6_txt_mlp_0.alpha: 1.0
 159. lora_unet_double_blocks_6_txt_mlp_2.alpha: 1.0
 160. lora_unet_double_blocks_6_txt_mod_lin.alpha: 1.0
 161. lora_unet_double_blocks_7_img_attn_proj.alpha: 1.0
 162. lora_unet_double_blocks_7_img_attn_qkv.alpha: 1.0
 163. lora_unet_double_blocks_7_img_mlp_0.alpha: 1.0
 164. lora_unet_double_blocks_7_img_mlp_2.alpha: 1.0
 165. lora_unet_double_blocks_7_img_mod_lin.alpha: 1.0
 166. lora_unet_double_blocks_7_txt_attn_proj.alpha: 1.0
 167. lora_unet_double_blocks_7_txt_attn_qkv.alpha: 1.0
 168. lora_unet_double_blocks_7_txt_mlp_0.alpha: 1.0
 169. lora_unet_double_blocks_7_txt_mlp_2.alpha: 1.0
 170. lora_unet_double_blocks_7_txt_mod_lin.alpha: 1.0
 171. lora_unet_double_blocks_8_img_attn_proj.alpha: 1.0
 172. lora_unet_double_blocks_8_img_attn_qkv.alpha: 1.0
 173. lora_unet_double_blocks_8_img_mlp_0.alpha: 1.0
 174. lora_unet_double_blocks_8_img_mlp_2.alpha: 1.0
 175. lora_unet_double_blocks_8_img_mod_lin.alpha: 1.0
 176. lora_unet_double_blocks_8_txt_attn_proj.alpha: 1.0
 177. lora_unet_double_blocks_8_txt_attn_qkv.alpha: 1.0
 178. lora_unet_double_blocks_8_txt_mlp_0.alpha: 1.0
 179. lora_unet_double_blocks_8_txt_mlp_2.alpha: 1.0
 180. lora_unet_double_blocks_8_txt_mod_lin.alpha: 1.0
 181. lora_unet_double_blocks_9_img_attn_proj.alpha: 1.0
 182. lora_unet_double_blocks_9_img_attn_qkv.alpha: 1.0
 183. lora_unet_double_blocks_9_img_mlp_0.alpha: 1.0
 184. lora_unet_double_blocks_9_img_mlp_2.alpha: 1.0
 185. lora_unet_double_blocks_9_img_mod_lin.alpha: 1.0
 186. lora_unet_double_blocks_9_txt_attn_proj.alpha: 1.0
 187. lora_unet_double_blocks_9_txt_attn_qkv.alpha: 1.0
 188. lora_unet_double_blocks_9_txt_mlp_0.alpha: 1.0
 189. lora_unet_double_blocks_9_txt_mlp_2.alpha: 1.0
 190. lora_unet_double_blocks_9_txt_mod_lin.alpha: 1.0
 191. lora_unet_single_blocks_0_linear1.alpha: 1.0
 192. lora_unet_single_blocks_0_linear2.alpha: 1.0
 193. lora_unet_single_blocks_0_modulation_lin.alpha: 1.0
 194. lora_unet_single_blocks_10_linear1.alpha: 1.0
 195. lora_unet_single_blocks_10_linear2.alpha: 1.0
 196. lora_unet_single_blocks_10_modulation_lin.alpha: 1.0
 197. lora_unet_single_blocks_11_linear1.alpha: 1.0
 198. lora_unet_single_blocks_11_linear2.alpha: 1.0
 199. lora_unet_single_blocks_11_modulation_lin.alpha: 1.0
 200. lora_unet_single_blocks_12_linear1.alpha: 1.0
 201. lora_unet_single_blocks_12_linear2.alpha: 1.0
 202. lora_unet_single_blocks_12_modulation_lin.alpha: 1.0
 203. lora_unet_single_blocks_13_linear1.alpha: 1.0
 204. lora_unet_single_blocks_13_linear2.alpha: 1.0
 205. lora_unet_single_blocks_13_modulation_lin.alpha: 1.0
 206. lora_unet_single_blocks_14_linear1.alpha: 1.0
 207. lora_unet_single_blocks_14_linear2.alpha: 1.0
 208. lora_unet_single_blocks_14_modulation_lin.alpha: 1.0
 209. lora_unet_single_blocks_15_linear1.alpha: 1.0
 210. lora_unet_single_blocks_15_linear2.alpha: 1.0
 211. lora_unet_single_blocks_15_modulation_lin.alpha: 1.0
 212. lora_unet_single_blocks_16_linear1.alpha: 1.0
 213. lora_unet_single_blocks_16_linear2.alpha: 1.0
 214. lora_unet_single_blocks_16_modulation_lin.alpha: 1.0
 215. lora_unet_single_blocks_17_linear1.alpha: 1.0
 216. lora_unet_single_blocks_17_linear2.alpha: 1.0
 217. lora_unet_single_blocks_17_modulation_lin.alpha: 1.0
 218. lora_unet_single_blocks_18_linear1.alpha: 1.0
 219. lora_unet_single_blocks_18_linear2.alpha: 1.0
 220. lora_unet_single_blocks_18_modulation_lin.alpha: 1.0
 221. lora_unet_single_blocks_19_linear1.alpha: 1.0
 222. lora_unet_single_blocks_19_linear2.alpha: 1.0
 223. lora_unet_single_blocks_19_modulation_lin.alpha: 1.0
 224. lora_unet_single_blocks_1_linear1.alpha: 1.0
 225. lora_unet_single_blocks_1_linear2.alpha: 1.0
 226. lora_unet_single_blocks_1_modulation_lin.alpha: 1.0
 227. lora_unet_single_blocks_20_linear1.alpha: 1.0
 228. lora_unet_single_blocks_20_linear2.alpha: 1.0
 229. lora_unet_single_blocks_20_modulation_lin.alpha: 1.0
 230. lora_unet_single_blocks_21_linear1.alpha: 1.0
 231. lora_unet_single_blocks_21_linear2.alpha: 1.0
 232. lora_unet_single_blocks_21_modulation_lin.alpha: 1.0
 233. lora_unet_single_blocks_22_linear1.alpha: 1.0
 234. lora_unet_single_blocks_22_linear2.alpha: 1.0
 235. lora_unet_single_blocks_22_modulation_lin.alpha: 1.0
 236. lora_unet_single_blocks_23_linear1.alpha: 1.0
 237. lora_unet_single_blocks_23_linear2.alpha: 1.0
 238. lora_unet_single_blocks_23_modulation_lin.alpha: 1.0
 239. lora_unet_single_blocks_24_linear1.alpha: 1.0
 240. lora_unet_single_blocks_24_linear2.alpha: 1.0
 241. lora_unet_single_blocks_24_modulation_lin.alpha: 1.0
 242. lora_unet_single_blocks_25_linear1.alpha: 1.0
 243. lora_unet_single_blocks_25_linear2.alpha: 1.0
 244. lora_unet_single_blocks_25_modulation_lin.alpha: 1.0
 245. lora_unet_single_blocks_26_linear1.alpha: 1.0
 246. lora_unet_single_blocks_26_linear2.alpha: 1.0
 247. lora_unet_single_blocks_26_modulation_lin.alpha: 1.0
 248. lora_unet_single_blocks_27_linear1.alpha: 1.0
 249. lora_unet_single_blocks_27_linear2.alpha: 1.0
 250. lora_unet_single_blocks_27_modulation_lin.alpha: 1.0
 251. lora_unet_single_blocks_28_linear1.alpha: 1.0
 252. lora_unet_single_blocks_28_linear2.alpha: 1.0
 253. lora_unet_single_blocks_28_modulation_lin.alpha: 1.0
 254. lora_unet_single_blocks_29_linear1.alpha: 1.0
 255. lora_unet_single_blocks_29_linear2.alpha: 1.0
 256. lora_unet_single_blocks_29_modulation_lin.alpha: 1.0
 257. lora_unet_single_blocks_2_linear1.alpha: 1.0
 258. lora_unet_single_blocks_2_linear2.alpha: 1.0
 259. lora_unet_single_blocks_2_modulation_lin.alpha: 1.0
 260. lora_unet_single_blocks_30_linear1.alpha: 1.0
 261. lora_unet_single_blocks_30_linear2.alpha: 1.0
 262. lora_unet_single_blocks_30_modulation_lin.alpha: 1.0
 263. lora_unet_single_blocks_31_linear1.alpha: 1.0
 264. lora_unet_single_blocks_31_linear2.alpha: 1.0
 265. lora_unet_single_blocks_31_modulation_lin.alpha: 1.0
 266. lora_unet_single_blocks_32_linear1.alpha: 1.0
 267. lora_unet_single_blocks_32_linear2.alpha: 1.0
 268. lora_unet_single_blocks_32_modulation_lin.alpha: 1.0
 269. lora_unet_single_blocks_33_linear1.alpha: 1.0
 270. lora_unet_single_blocks_33_linear2.alpha: 1.0
 271. lora_unet_single_blocks_33_modulation_lin.alpha: 1.0
 272. lora_unet_single_blocks_34_linear1.alpha: 1.0
 273. lora_unet_single_blocks_34_linear2.alpha: 1.0
 274. lora_unet_single_blocks_34_modulation_lin.alpha: 1.0
 275. lora_unet_single_blocks_35_linear1.alpha: 1.0
 276. lora_unet_single_blocks_35_linear2.alpha: 1.0
 277. lora_unet_single_blocks_35_modulation_lin.alpha: 1.0
 278. lora_unet_single_blocks_36_linear1.alpha: 1.0
 279. lora_unet_single_blocks_36_linear2.alpha: 1.0
 280. lora_unet_single_blocks_36_modulation_lin.alpha: 1.0
 281. lora_unet_single_blocks_37_linear1.alpha: 1.0
 282. lora_unet_single_blocks_37_linear2.alpha: 1.0
 283. lora_unet_single_blocks_37_modulation_lin.alpha: 1.0
 284. lora_unet_single_blocks_3_linear1.alpha: 1.0
 285. lora_unet_single_blocks_3_linear2.alpha: 1.0
 286. lora_unet_single_blocks_3_modulation_lin.alpha: 1.0
 287. lora_unet_single_blocks_4_linear1.alpha: 1.0
 288. lora_unet_single_blocks_4_linear2.alpha: 1.0
 289. lora_unet_single_blocks_4_modulation_lin.alpha: 1.0
 290. lora_unet_single_blocks_5_linear1.alpha: 1.0
 291. lora_unet_single_blocks_5_linear2.alpha: 1.0
 292. lora_unet_single_blocks_5_modulation_lin.alpha: 1.0
 293. lora_unet_single_blocks_6_linear1.alpha: 1.0
 294. lora_unet_single_blocks_6_linear2.alpha: 1.0
 295. lora_unet_single_blocks_6_modulation_lin.alpha: 1.0
 296. lora_unet_single_blocks_7_linear1.alpha: 1.0
 297. lora_unet_single_blocks_7_linear2.alpha: 1.0
 298. lora_unet_single_blocks_7_modulation_lin.alpha: 1.0
 299. lora_unet_single_blocks_8_linear1.alpha: 1.0
 300. lora_unet_single_blocks_8_linear2.alpha: 1.0
 301. lora_unet_single_blocks_8_modulation_lin.alpha: 1.0
 302. lora_unet_single_blocks_9_linear1.alpha: 1.0
 303. lora_unet_single_blocks_9_linear2.alpha: 1.0
 304. lora_unet_single_blocks_9_modulation_lin.alpha: 1.0
Found 608 potential LoRA/Adapter components:

lora_unet_double_blocks_0_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.792248, 2.331898, 1.786346, 0.777732]
  full_rank: 4

lora_unet_double_blocks_0_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.854153, 3.893011, 3.035948, 0.951476]
  full_rank: 4

lora_unet_double_blocks_0_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.048511, 2.227099, 1.547429, 0.907319]
  full_rank: 4

lora_unet_double_blocks_0_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.405525, 6.764925, 4.992197, 2.123711]
  full_rank: 4

lora_unet_double_blocks_0_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.367493, 2.329315, 2.171223, 2.073421]
  full_rank: 4

lora_unet_double_blocks_0_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.382353, 7.249229, 6.454361, 6.093140]
  full_rank: 4

lora_unet_double_blocks_0_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [9.503619, 6.536604, 5.456003, 4.288830]
  full_rank: 4

lora_unet_double_blocks_0_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.656897, 4.480735, 4.036865, 3.360387]
  full_rank: 4

lora_unet_double_blocks_0_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.483803, 1.887090, 1.468503, 1.125265]
  full_rank: 4

lora_unet_double_blocks_0_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.638390, 9.608647, 8.019947, 5.695004]
  full_rank: 4

lora_unet_double_blocks_0_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.153553, 4.288892, 2.845954, 2.171001]
  full_rank: 4

lora_unet_double_blocks_0_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.723092, 3.784182, 2.817077, 1.721529]
  full_rank: 4

lora_unet_double_blocks_0_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.880370, 4.145465, 1.125278, 1.047175]
  full_rank: 4

lora_unet_double_blocks_0_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.849163, 8.608264, 1.892147, 1.551859]
  full_rank: 4

lora_unet_double_blocks_0_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.250703, 3.401339, 1.532924, 1.321772]
  full_rank: 4

lora_unet_double_blocks_0_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.242718, 6.426779, 3.986952, 2.998836]
  full_rank: 4

lora_unet_double_blocks_0_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [16.124371, 9.738408, 6.381729, 5.289555]
  full_rank: 4

lora_unet_double_blocks_0_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.458300, 4.799097, 3.285149, 2.628419]
  full_rank: 4

lora_unet_double_blocks_0_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.780369, 1.533444, 1.328598, 0.622996]
  full_rank: 4

lora_unet_double_blocks_0_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.199005, 11.118936, 7.054936, 3.938051]
  full_rank: 4

lora_unet_double_blocks_10_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.038676, 5.088492, 3.954862, 2.878361]
  full_rank: 4

lora_unet_double_blocks_10_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.595389, 5.585897, 4.230687, 3.280075]
  full_rank: 4

lora_unet_double_blocks_10_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.036952, 4.987420, 4.403517, 2.113069]
  full_rank: 4

lora_unet_double_blocks_10_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [10.661877, 10.135051, 8.482531, 3.958704]
  full_rank: 4

lora_unet_double_blocks_10_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.205013, 5.808718, 2.543340, 1.829898]
  full_rank: 4

lora_unet_double_blocks_10_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.828177, 12.304207, 6.126333, 4.340699]
  full_rank: 4

lora_unet_double_blocks_10_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [16.497168, 10.546461, 6.299367, 3.445177]
  full_rank: 4

lora_unet_double_blocks_10_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.337024, 5.436263, 3.216852, 1.791851]
  full_rank: 4

lora_unet_double_blocks_10_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.711455, 2.225858, 1.431428, 0.698389]
  full_rank: 4

lora_unet_double_blocks_10_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.996054, 10.631987, 6.523725, 4.718598]
  full_rank: 4

lora_unet_double_blocks_10_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.031947, 5.358431, 2.753309, 0.769907]
  full_rank: 4

lora_unet_double_blocks_10_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.983306, 6.297292, 2.734956, 0.609213]
  full_rank: 4

lora_unet_double_blocks_10_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.817083, 4.892073, 2.525749, 1.403988]
  full_rank: 4

lora_unet_double_blocks_10_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.608500, 9.036184, 5.179103, 2.407380]
  full_rank: 4

lora_unet_double_blocks_10_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.778072, 4.088052, 3.041368, 1.514337]
  full_rank: 4

lora_unet_double_blocks_10_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.334244, 6.030541, 5.038105, 2.460872]
  full_rank: 4

lora_unet_double_blocks_10_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.864322, 7.180579, 5.366474, 3.362918]
  full_rank: 4

lora_unet_double_blocks_10_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.060049, 5.702049, 4.354019, 2.496794]
  full_rank: 4

lora_unet_double_blocks_10_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.497052, 0.608430, 0.513541, 0.506174]
  full_rank: 4

lora_unet_double_blocks_10_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.417606, 1.922773, 0.431201, 0.377151]
  full_rank: 4

lora_unet_double_blocks_11_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.197822, 3.765102, 3.202411, 1.986153]
  full_rank: 4

lora_unet_double_blocks_11_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.822280, 4.319606, 3.356708, 2.087052]
  full_rank: 4

lora_unet_double_blocks_11_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.065437, 4.109492, 2.587250, 2.326356]
  full_rank: 4

lora_unet_double_blocks_11_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.040816, 7.939018, 5.344254, 4.156125]
  full_rank: 4

lora_unet_double_blocks_11_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.445547, 4.843488, 2.162680, 1.672261]
  full_rank: 4

lora_unet_double_blocks_11_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.665520, 11.337801, 5.621830, 3.235411]
  full_rank: 4

lora_unet_double_blocks_11_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.431055, 8.783831, 5.176537, 2.954219]
  full_rank: 4

lora_unet_double_blocks_11_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.444018, 4.649412, 2.630003, 1.597999]
  full_rank: 4

lora_unet_double_blocks_11_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.928029, 2.077049, 1.513653, 0.630706]
  full_rank: 4

lora_unet_double_blocks_11_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.271732, 12.641569, 8.995202, 3.805614]
  full_rank: 4

lora_unet_double_blocks_11_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.584460, 3.808267, 2.476887, 1.184775]
  full_rank: 4

lora_unet_double_blocks_11_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.959812, 4.257248, 2.489618, 1.167176]
  full_rank: 4

lora_unet_double_blocks_11_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.428980, 4.018283, 2.424583, 1.432512]
  full_rank: 4

lora_unet_double_blocks_11_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.087037, 7.685914, 3.946429, 2.316015]
  full_rank: 4

lora_unet_double_blocks_11_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.133273, 0.872441, 0.680047, 0.546837]
  full_rank: 4

lora_unet_double_blocks_11_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.442036, 2.494263, 0.790933, 0.476101]
  full_rank: 4

lora_unet_double_blocks_11_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [13.750122, 8.036111, 5.793415, 3.796773]
  full_rank: 4

lora_unet_double_blocks_11_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.402003, 5.368621, 3.809469, 2.528645]
  full_rank: 4

lora_unet_double_blocks_11_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.378171, 1.272819, 0.875857, 0.583868]
  full_rank: 4

lora_unet_double_blocks_11_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.361277, 9.409139, 4.978188, 1.890774]
  full_rank: 4

lora_unet_double_blocks_12_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.049644, 5.369306, 3.631473, 2.878226]
  full_rank: 4

lora_unet_double_blocks_12_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.526815, 5.639666, 4.011107, 3.024501]
  full_rank: 4

lora_unet_double_blocks_12_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.542114, 4.686868, 3.056760, 2.003160]
  full_rank: 4

lora_unet_double_blocks_12_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.898998, 8.786354, 6.314941, 3.911224]
  full_rank: 4

lora_unet_double_blocks_12_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.071695, 3.692238, 2.639697, 1.833205]
  full_rank: 4

lora_unet_double_blocks_12_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.164816, 9.117884, 6.314018, 4.275924]
  full_rank: 4

lora_unet_double_blocks_12_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.509686, 10.749178, 7.107432, 6.602817]
  full_rank: 4

lora_unet_double_blocks_12_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.857935, 6.059021, 3.706646, 3.645354]
  full_rank: 4

lora_unet_double_blocks_12_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.284767, 2.721200, 1.719932, 0.641865]
  full_rank: 4

lora_unet_double_blocks_12_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.809826, 11.562099, 9.950126, 4.046813]
  full_rank: 4

lora_unet_double_blocks_12_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.863597, 3.522622, 1.953988, 1.426020]
  full_rank: 4

lora_unet_double_blocks_12_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.399984, 3.561695, 1.830844, 1.311303]
  full_rank: 4

lora_unet_double_blocks_12_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.124494, 4.537672, 3.715697, 1.912649]
  full_rank: 4

lora_unet_double_blocks_12_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.437026, 9.212364, 8.035363, 3.977262]
  full_rank: 4

lora_unet_double_blocks_12_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.698485, 1.500311, 1.091943, 0.627845]
  full_rank: 4

lora_unet_double_blocks_12_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.931312, 2.224271, 0.750343, 0.460508]
  full_rank: 4

lora_unet_double_blocks_12_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.843294, 3.950375, 2.099157, 1.458173]
  full_rank: 4

lora_unet_double_blocks_12_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.376220, 5.745668, 2.864326, 1.752460]
  full_rank: 4

lora_unet_double_blocks_12_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.637929, 1.062154, 0.531492, 0.507492]
  full_rank: 4

lora_unet_double_blocks_12_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.620543, 7.050877, 1.026236, 0.413221]
  full_rank: 4

lora_unet_double_blocks_13_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.846461, 4.579978, 4.076528, 2.285181]
  full_rank: 4

lora_unet_double_blocks_13_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.655683, 4.830821, 4.411818, 2.407937]
  full_rank: 4

lora_unet_double_blocks_13_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.126799, 4.101981, 3.733178, 1.717392]
  full_rank: 4

lora_unet_double_blocks_13_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.837235, 7.960819, 7.324419, 3.355902]
  full_rank: 4

lora_unet_double_blocks_13_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.223946, 5.176298, 4.124957, 1.048282]
  full_rank: 4

lora_unet_double_blocks_13_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.630889, 11.195914, 9.778944, 1.983341]
  full_rank: 4

lora_unet_double_blocks_13_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.294233, 9.225713, 5.025873, 2.809050]
  full_rank: 4

lora_unet_double_blocks_13_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.194653, 4.838957, 2.980316, 1.457006]
  full_rank: 4

lora_unet_double_blocks_13_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.618533, 2.650025, 0.725900, 0.534944]
  full_rank: 4

lora_unet_double_blocks_13_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.462797, 13.737528, 4.406012, 1.655695]
  full_rank: 4

lora_unet_double_blocks_13_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [9.110583, 2.456220, 1.228824, 0.620901]
  full_rank: 4

lora_unet_double_blocks_13_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.289995, 2.592642, 1.074351, 0.383880]
  full_rank: 4

lora_unet_double_blocks_13_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.551534, 4.792109, 4.166527, 3.511004]
  full_rank: 4

lora_unet_double_blocks_13_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.394121, 9.273672, 6.876315, 5.879333]
  full_rank: 4

lora_unet_double_blocks_13_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.530027, 4.279636, 3.094183, 1.916537]
  full_rank: 4

lora_unet_double_blocks_13_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.816427, 4.569152, 3.498119, 2.279934]
  full_rank: 4

lora_unet_double_blocks_13_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.807812, 4.201373, 3.337842, 1.379820]
  full_rank: 4

lora_unet_double_blocks_13_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.852132, 5.878225, 3.622045, 1.520996]
  full_rank: 4

lora_unet_double_blocks_13_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.545050, 0.817055, 0.532786, 0.510217]
  full_rank: 4

lora_unet_double_blocks_13_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.790119, 3.605129, 1.151883, 0.360871]
  full_rank: 4

lora_unet_double_blocks_14_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.500019, 4.907355, 4.417390, 2.636585]
  full_rank: 4

lora_unet_double_blocks_14_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.023273, 5.451813, 4.972558, 3.210082]
  full_rank: 4

lora_unet_double_blocks_14_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.874213, 4.878154, 4.234645, 1.964306]
  full_rank: 4

lora_unet_double_blocks_14_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [10.977302, 9.228429, 7.943727, 3.647003]
  full_rank: 4

lora_unet_double_blocks_14_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.368652, 4.620236, 3.246955, 1.425514]
  full_rank: 4

lora_unet_double_blocks_14_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.369263, 10.285238, 7.622007, 2.744124]
  full_rank: 4

lora_unet_double_blocks_14_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [16.077942, 9.971864, 5.917325, 3.458398]
  full_rank: 4

lora_unet_double_blocks_14_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.640193, 4.894183, 3.710768, 1.866204]
  full_rank: 4

lora_unet_double_blocks_14_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.499095, 1.991557, 0.951848, 0.517682]
  full_rank: 4

lora_unet_double_blocks_14_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.980925, 10.344007, 5.399458, 1.084706]
  full_rank: 4

lora_unet_double_blocks_14_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [9.322334, 4.963622, 1.780933, 0.676931]
  full_rank: 4

lora_unet_double_blocks_14_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.500008, 4.804495, 1.897497, 0.512850]
  full_rank: 4

lora_unet_double_blocks_14_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.123928, 3.847754, 1.120044, 0.902412]
  full_rank: 4

lora_unet_double_blocks_14_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.390185, 7.665846, 1.696654, 1.463866]
  full_rank: 4

lora_unet_double_blocks_14_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.333778, 3.563443, 2.613862, 1.462612]
  full_rank: 4

lora_unet_double_blocks_14_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.108668, 5.291297, 3.295991, 1.464110]
  full_rank: 4

lora_unet_double_blocks_14_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.173374, 3.105830, 1.620032, 1.110833]
  full_rank: 4

lora_unet_double_blocks_14_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.457524, 3.639873, 1.711289, 1.101902]
  full_rank: 4

lora_unet_double_blocks_14_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.377667, 1.959164, 0.789559, 0.509122]
  full_rank: 4

lora_unet_double_blocks_14_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.621408, 9.644314, 4.002599, 0.413700]
  full_rank: 4

lora_unet_double_blocks_15_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.616839, 5.278351, 2.764623, 2.378181]
  full_rank: 4

lora_unet_double_blocks_15_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.218675, 5.977190, 3.070082, 2.444897]
  full_rank: 4

lora_unet_double_blocks_15_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.646737, 4.498806, 2.930382, 1.778117]
  full_rank: 4

lora_unet_double_blocks_15_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.663704, 8.733545, 5.759778, 3.310774]
  full_rank: 4

lora_unet_double_blocks_15_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.401015, 4.614120, 4.488917, 2.160328]
  full_rank: 4

lora_unet_double_blocks_15_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.713600, 10.881257, 10.193756, 4.836057]
  full_rank: 4

lora_unet_double_blocks_15_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [18.601528, 5.167692, 3.671178, 2.774128]
  full_rank: 4

lora_unet_double_blocks_15_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.107025, 3.285353, 2.230499, 1.604414]
  full_rank: 4

lora_unet_double_blocks_15_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.939019, 1.127122, 0.886300, 0.555053]
  full_rank: 4

lora_unet_double_blocks_15_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.958891, 7.405316, 4.299945, 1.604245]
  full_rank: 4

lora_unet_double_blocks_15_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [9.263738, 3.180891, 1.247290, 0.915850]
  full_rank: 4

lora_unet_double_blocks_15_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.859856, 2.804933, 1.036397, 0.716196]
  full_rank: 4

lora_unet_double_blocks_15_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.539264, 5.368559, 3.077834, 2.238698]
  full_rank: 4

lora_unet_double_blocks_15_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.316123, 10.398317, 6.891246, 4.351301]
  full_rank: 4

lora_unet_double_blocks_15_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.864285, 3.684552, 1.062929, 0.624018]
  full_rank: 4

lora_unet_double_blocks_15_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.044645, 9.056532, 3.293589, 1.053050]
  full_rank: 4

lora_unet_double_blocks_15_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [18.162291, 6.612812, 2.595296, 1.535457]
  full_rank: 4

lora_unet_double_blocks_15_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.833566, 3.478822, 1.398758, 0.850925]
  full_rank: 4

lora_unet_double_blocks_15_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.445481, 0.920502, 0.568137, 0.517318]
  full_rank: 4

lora_unet_double_blocks_15_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.893629, 4.669659, 1.916134, 0.594368]
  full_rank: 4

lora_unet_double_blocks_16_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.412085, 4.675937, 3.572840, 2.143737]
  full_rank: 4

lora_unet_double_blocks_16_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.045751, 5.177007, 4.028530, 2.589797]
  full_rank: 4

lora_unet_double_blocks_16_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.721983, 4.774340, 3.889859, 2.796137]
  full_rank: 4

lora_unet_double_blocks_16_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [10.653697, 9.706388, 7.377531, 5.618845]
  full_rank: 4

lora_unet_double_blocks_16_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.621249, 4.626348, 2.868621, 2.630955]
  full_rank: 4

lora_unet_double_blocks_16_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.904222, 10.768465, 7.044238, 6.240102]
  full_rank: 4

lora_unet_double_blocks_16_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [13.757896, 11.124875, 8.147998, 6.340936]
  full_rank: 4

lora_unet_double_blocks_16_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.030958, 5.725687, 4.415090, 3.571693]
  full_rank: 4

lora_unet_double_blocks_16_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.039714, 2.554370, 1.909745, 0.690264]
  full_rank: 4

lora_unet_double_blocks_16_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.272169, 12.218868, 9.251003, 3.084623]
  full_rank: 4

lora_unet_double_blocks_16_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.288548, 2.967077, 1.180726, 0.983345]
  full_rank: 4

lora_unet_double_blocks_16_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.562099, 3.168440, 1.128744, 0.861949]
  full_rank: 4

lora_unet_double_blocks_16_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.252248, 4.552904, 2.672042, 1.995092]
  full_rank: 4

lora_unet_double_blocks_16_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.583036, 9.477307, 6.008618, 3.666653]
  full_rank: 4

lora_unet_double_blocks_16_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.267727, 1.521592, 0.636567, 0.602299]
  full_rank: 4

lora_unet_double_blocks_16_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.489567, 5.213800, 1.399391, 0.998785]
  full_rank: 4

lora_unet_double_blocks_16_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.121945, 11.237116, 7.782757, 4.525489]
  full_rank: 4

lora_unet_double_blocks_16_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.759416, 6.692531, 4.293304, 2.455705]
  full_rank: 4

lora_unet_double_blocks_16_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.515310, 2.216298, 1.290842, 0.617998]
  full_rank: 4

lora_unet_double_blocks_16_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.448793, 11.206985, 7.648530, 2.103011]
  full_rank: 4

lora_unet_double_blocks_17_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.623838, 4.130921, 2.218570, 2.179447]
  full_rank: 4

lora_unet_double_blocks_17_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.662589, 4.493361, 2.515911, 2.320039]
  full_rank: 4

lora_unet_double_blocks_17_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.265485, 4.306260, 3.261634, 1.438836]
  full_rank: 4

lora_unet_double_blocks_17_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.412091, 9.096026, 6.568718, 2.605109]
  full_rank: 4

lora_unet_double_blocks_17_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.376533, 4.555508, 3.826957, 2.659176]
  full_rank: 4

lora_unet_double_blocks_17_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.017068, 11.508709, 8.898104, 6.318570]
  full_rank: 4

lora_unet_double_blocks_17_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.035313, 8.875680, 8.604902, 5.990431]
  full_rank: 4

lora_unet_double_blocks_17_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.600734, 4.946567, 4.238675, 3.273928]
  full_rank: 4

lora_unet_double_blocks_17_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.141159, 1.666066, 1.025805, 0.670730]
  full_rank: 4

lora_unet_double_blocks_17_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.307301, 10.414664, 5.392419, 2.969849]
  full_rank: 4

lora_unet_double_blocks_17_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.531533, 2.992771, 2.098383, 1.011167]
  full_rank: 4

lora_unet_double_blocks_17_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.993402, 3.513705, 2.127918, 0.966424]
  full_rank: 4

lora_unet_double_blocks_17_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.493969, 4.109480, 1.648983, 1.235182]
  full_rank: 4

lora_unet_double_blocks_17_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.544388, 8.608583, 3.299980, 2.660389]
  full_rank: 4

lora_unet_double_blocks_17_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.659127, 3.842230, 1.301284, 0.747221]
  full_rank: 4

lora_unet_double_blocks_17_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.199934, 9.550814, 2.927498, 1.695113]
  full_rank: 4

lora_unet_double_blocks_17_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [19.349968, 6.204078, 4.147634, 1.663321]
  full_rank: 4

lora_unet_double_blocks_17_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.815628, 2.975633, 1.998735, 0.715683]
  full_rank: 4

lora_unet_double_blocks_17_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.064088, 1.204469, 0.584023, 0.515214]
  full_rank: 4

lora_unet_double_blocks_17_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.199076, 6.959852, 2.801348, 0.842587]
  full_rank: 4

lora_unet_double_blocks_18_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.821696, 2.846938, 2.329089, 1.563278]
  full_rank: 4

lora_unet_double_blocks_18_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.916306, 3.383523, 2.443489, 1.702516]
  full_rank: 4

lora_unet_double_blocks_18_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.733463, 3.707459, 2.681177, 1.774867]
  full_rank: 4

lora_unet_double_blocks_18_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.068981, 7.266051, 4.922919, 3.291653]
  full_rank: 4

lora_unet_double_blocks_18_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.573649, 3.994489, 3.439982, 1.602858]
  full_rank: 4

lora_unet_double_blocks_18_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.764860, 10.367939, 8.290976, 3.725134]
  full_rank: 4

lora_unet_double_blocks_18_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.460921, 9.899841, 8.726044, 5.673779]
  full_rank: 4

lora_unet_double_blocks_18_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.337050, 5.137397, 4.419785, 3.060136]
  full_rank: 4

lora_unet_double_blocks_18_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.061942, 1.919495, 1.420956, 0.744151]
  full_rank: 4

lora_unet_double_blocks_18_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.973982, 12.939407, 6.678848, 4.317547]
  full_rank: 4

lora_unet_double_blocks_18_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.105765, 4.597147, 3.670037, 1.809763]
  full_rank: 4

lora_unet_double_blocks_18_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.670821, 5.076095, 3.889683, 2.119156]
  full_rank: 4

lora_unet_double_blocks_18_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.358056, 3.694596, 2.574979, 1.604221]
  full_rank: 4

lora_unet_double_blocks_18_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.620137, 8.079520, 5.694506, 2.942733]
  full_rank: 4

lora_unet_double_blocks_18_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.395078, 4.481357, 2.735184, 1.437879]
  full_rank: 4

lora_unet_double_blocks_18_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.290462, 8.918783, 4.755687, 1.817251]
  full_rank: 4

lora_unet_double_blocks_18_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.523254, 11.390183, 5.432096, 4.555151]
  full_rank: 4

lora_unet_double_blocks_18_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.798492, 6.267749, 3.394976, 2.516589]
  full_rank: 4

lora_unet_double_blocks_18_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.880669, 2.271691, 1.860842, 1.153096]
  full_rank: 4

lora_unet_double_blocks_18_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.341067, 11.316048, 9.590265, 6.179044]
  full_rank: 4

lora_unet_double_blocks_1_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.727190, 3.078500, 2.864677, 1.887856]
  full_rank: 4

lora_unet_double_blocks_1_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.084115, 4.622778, 3.896765, 2.754091]
  full_rank: 4

lora_unet_double_blocks_1_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.243438, 2.369627, 1.896587, 1.505046]
  full_rank: 4

lora_unet_double_blocks_1_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.798561, 7.751334, 4.801297, 3.939539]
  full_rank: 4

lora_unet_double_blocks_1_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.051765, 2.001329, 1.924748, 1.744620]
  full_rank: 4

lora_unet_double_blocks_1_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.314948, 5.654131, 5.339592, 4.768227]
  full_rank: 4

lora_unet_double_blocks_1_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [9.508054, 6.871140, 6.504551, 5.074590]
  full_rank: 4

lora_unet_double_blocks_1_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.057967, 4.836524, 4.228691, 3.651592]
  full_rank: 4

lora_unet_double_blocks_1_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [1.957035, 1.628505, 1.461665, 1.184032]
  full_rank: 4

lora_unet_double_blocks_1_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.592247, 10.048994, 8.397002, 7.465239]
  full_rank: 4

lora_unet_double_blocks_1_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.112668, 4.417605, 1.819438, 1.254086]
  full_rank: 4

lora_unet_double_blocks_1_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.027347, 4.755090, 1.688476, 1.326642]
  full_rank: 4

lora_unet_double_blocks_1_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.831470, 3.256065, 1.809496, 0.823455]
  full_rank: 4

lora_unet_double_blocks_1_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.346173, 7.103209, 3.092997, 1.481983]
  full_rank: 4

lora_unet_double_blocks_1_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.247057, 4.410502, 2.789483, 2.275627]
  full_rank: 4

lora_unet_double_blocks_1_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.067816, 6.784544, 4.583248, 3.268080]
  full_rank: 4

lora_unet_double_blocks_1_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [9.405028, 7.115264, 4.215509, 2.277060]
  full_rank: 4

lora_unet_double_blocks_1_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.644680, 5.741961, 3.302071, 1.662948]
  full_rank: 4

lora_unet_double_blocks_1_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.168037, 2.113857, 1.170283, 0.514001]
  full_rank: 4

lora_unet_double_blocks_1_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.368706, 9.775669, 7.887975, 0.677521]
  full_rank: 4

lora_unet_double_blocks_2_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.572556, 4.266208, 2.450098, 1.898292]
  full_rank: 4

lora_unet_double_blocks_2_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.128513, 4.975835, 2.995283, 1.995205]
  full_rank: 4

lora_unet_double_blocks_2_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.989506, 2.662087, 1.932147, 1.007323]
  full_rank: 4

lora_unet_double_blocks_2_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.600027, 7.613403, 5.958717, 2.171025]
  full_rank: 4

lora_unet_double_blocks_2_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.201676, 2.284085, 2.078754, 1.863793]
  full_rank: 4

lora_unet_double_blocks_2_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.723218, 5.396408, 4.820347, 3.691754]
  full_rank: 4

lora_unet_double_blocks_2_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.167320, 9.514547, 7.493992, 5.320179]
  full_rank: 4

lora_unet_double_blocks_2_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.429400, 5.548048, 4.516753, 3.328478]
  full_rank: 4

lora_unet_double_blocks_2_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.283761, 1.564580, 1.413416, 1.122949]
  full_rank: 4

lora_unet_double_blocks_2_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.991280, 9.902222, 6.440149, 4.971840]
  full_rank: 4

lora_unet_double_blocks_2_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.038499, 4.132555, 1.386039, 0.705426]
  full_rank: 4

lora_unet_double_blocks_2_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.996211, 4.404796, 1.369138, 0.439890]
  full_rank: 4

lora_unet_double_blocks_2_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.910100, 0.864733, 0.763866, 0.567165]
  full_rank: 4

lora_unet_double_blocks_2_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.243489, 1.863767, 1.364920, 0.630564]
  full_rank: 4

lora_unet_double_blocks_2_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.170567, 4.372698, 2.396835, 1.604784]
  full_rank: 4

lora_unet_double_blocks_2_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.016127, 6.184356, 3.477509, 2.539686]
  full_rank: 4

lora_unet_double_blocks_2_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [11.879395, 7.212644, 3.589681, 1.698556]
  full_rank: 4

lora_unet_double_blocks_2_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.282619, 5.335476, 3.037425, 1.391172]
  full_rank: 4

lora_unet_double_blocks_2_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.375556, 0.738181, 0.535584, 0.511072]
  full_rank: 4

lora_unet_double_blocks_2_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.616589, 2.887513, 1.938285, 0.270071]
  full_rank: 4

lora_unet_double_blocks_3_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.147703, 4.537240, 3.031603, 2.341911]
  full_rank: 4

lora_unet_double_blocks_3_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.907125, 5.996002, 3.577862, 2.522549]
  full_rank: 4

lora_unet_double_blocks_3_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.569716, 3.284184, 2.108232, 1.425018]
  full_rank: 4

lora_unet_double_blocks_3_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.638426, 8.726351, 5.156444, 3.113430]
  full_rank: 4

lora_unet_double_blocks_3_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.133233, 3.743394, 3.501840, 1.974301]
  full_rank: 4

lora_unet_double_blocks_3_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.649652, 8.458673, 8.024409, 4.870750]
  full_rank: 4

lora_unet_double_blocks_3_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [12.376268, 9.788883, 5.931596, 3.315942]
  full_rank: 4

lora_unet_double_blocks_3_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.197824, 5.611638, 3.288022, 1.810132]
  full_rank: 4

lora_unet_double_blocks_3_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.571389, 1.858791, 0.851401, 0.601062]
  full_rank: 4

lora_unet_double_blocks_3_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.372890, 12.296825, 4.305265, 2.173018]
  full_rank: 4

lora_unet_double_blocks_3_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.996233, 2.421156, 0.939953, 0.583581]
  full_rank: 4

lora_unet_double_blocks_3_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.882586, 2.272660, 0.658439, 0.251752]
  full_rank: 4

lora_unet_double_blocks_3_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.317082, 3.971280, 3.238455, 1.125655]
  full_rank: 4

lora_unet_double_blocks_3_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.957846, 8.804297, 6.807821, 2.201186]
  full_rank: 4

lora_unet_double_blocks_3_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.147810, 3.186811, 2.775500, 1.046718]
  full_rank: 4

lora_unet_double_blocks_3_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.124796, 5.637760, 5.272576, 2.084620]
  full_rank: 4

lora_unet_double_blocks_3_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [12.720526, 8.173324, 4.136556, 2.572979]
  full_rank: 4

lora_unet_double_blocks_3_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.154540, 5.489829, 2.894645, 1.549204]
  full_rank: 4

lora_unet_double_blocks_3_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.590062, 1.376325, 0.731421, 0.536518]
  full_rank: 4

lora_unet_double_blocks_3_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.488750, 8.610884, 3.360333, 1.235116]
  full_rank: 4

lora_unet_double_blocks_4_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.791889, 5.439859, 3.113046, 2.632022]
  full_rank: 4

lora_unet_double_blocks_4_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.402979, 5.087018, 3.494635, 3.087579]
  full_rank: 4

lora_unet_double_blocks_4_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.053504, 3.721506, 2.408142, 1.522993]
  full_rank: 4

lora_unet_double_blocks_4_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.235175, 7.862825, 5.144731, 3.247401]
  full_rank: 4

lora_unet_double_blocks_4_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.029700, 3.755684, 1.813682, 1.161141]
  full_rank: 4

lora_unet_double_blocks_4_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.175764, 10.009457, 3.989217, 2.309334]
  full_rank: 4

lora_unet_double_blocks_4_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.404087, 11.537303, 7.329152, 5.295643]
  full_rank: 4

lora_unet_double_blocks_4_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.475611, 5.988497, 3.682000, 3.196817]
  full_rank: 4

lora_unet_double_blocks_4_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.544224, 2.400440, 1.192754, 0.545495]
  full_rank: 4

lora_unet_double_blocks_4_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.428242, 10.906276, 8.866471, 1.596651]
  full_rank: 4

lora_unet_double_blocks_4_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.336040, 2.481400, 0.816981, 0.606544]
  full_rank: 4

lora_unet_double_blocks_4_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.937152, 2.402750, 0.644163, 0.332991]
  full_rank: 4

lora_unet_double_blocks_4_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.020691, 4.875740, 3.473573, 2.510448]
  full_rank: 4

lora_unet_double_blocks_4_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [10.403840, 10.310276, 7.098327, 5.423841]
  full_rank: 4

lora_unet_double_blocks_4_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.150527, 1.861947, 0.803711, 0.700453]
  full_rank: 4

lora_unet_double_blocks_4_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.529476, 3.826681, 1.323764, 0.930122]
  full_rank: 4

lora_unet_double_blocks_4_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [11.408462, 8.344794, 4.143980, 2.290205]
  full_rank: 4

lora_unet_double_blocks_4_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.727723, 5.496049, 2.970028, 1.528495]
  full_rank: 4

lora_unet_double_blocks_4_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.402290, 2.054005, 1.312491, 0.779166]
  full_rank: 4

lora_unet_double_blocks_4_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.193422, 11.898224, 7.669388, 4.555955]
  full_rank: 4

lora_unet_double_blocks_5_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.456641, 4.084604, 2.704185, 1.682369]
  full_rank: 4

lora_unet_double_blocks_5_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.469101, 4.280472, 3.011013, 1.701035]
  full_rank: 4

lora_unet_double_blocks_5_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.683983, 2.710078, 2.000659, 1.094608]
  full_rank: 4

lora_unet_double_blocks_5_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.217186, 5.736606, 4.305171, 1.972161]
  full_rank: 4

lora_unet_double_blocks_5_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.928392, 3.362743, 1.758111, 0.731918]
  full_rank: 4

lora_unet_double_blocks_5_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.437898, 8.178397, 4.621995, 1.566576]
  full_rank: 4

lora_unet_double_blocks_5_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.696732, 6.705672, 4.781018, 2.786258]
  full_rank: 4

lora_unet_double_blocks_5_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.115883, 4.658463, 2.349428, 1.668492]
  full_rank: 4

lora_unet_double_blocks_5_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.172400, 1.210239, 0.623430, 0.566270]
  full_rank: 4

lora_unet_double_blocks_5_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.055178, 9.031359, 3.023894, 2.112268]
  full_rank: 4

lora_unet_double_blocks_5_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.789880, 3.695011, 0.944879, 0.662424]
  full_rank: 4

lora_unet_double_blocks_5_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.581128, 3.701807, 0.814832, 0.373584]
  full_rank: 4

lora_unet_double_blocks_5_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.672625, 4.117305, 3.534822, 2.404794]
  full_rank: 4

lora_unet_double_blocks_5_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.066256, 8.702004, 7.143412, 4.983110]
  full_rank: 4

lora_unet_double_blocks_5_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.266907, 0.905758, 0.691771, 0.689212]
  full_rank: 4

lora_unet_double_blocks_5_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.132572, 1.822096, 1.090959, 1.015942]
  full_rank: 4

lora_unet_double_blocks_5_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.523807, 5.412351, 3.184781, 1.992379]
  full_rank: 4

lora_unet_double_blocks_5_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.002999, 3.272299, 1.803092, 1.180591]
  full_rank: 4

lora_unet_double_blocks_5_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.692482, 0.851522, 0.538682, 0.511480]
  full_rank: 4

lora_unet_double_blocks_5_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.413492, 4.127575, 1.376014, 0.885957]
  full_rank: 4

lora_unet_double_blocks_6_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.476080, 3.195546, 2.838557, 1.217836]
  full_rank: 4

lora_unet_double_blocks_6_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.300024, 3.701174, 3.169818, 1.142450]
  full_rank: 4

lora_unet_double_blocks_6_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.486414, 2.415331, 1.288833, 1.026868]
  full_rank: 4

lora_unet_double_blocks_6_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.179501, 4.807032, 2.487936, 1.767911]
  full_rank: 4

lora_unet_double_blocks_6_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.596467, 3.889524, 1.359771, 1.242061]
  full_rank: 4

lora_unet_double_blocks_6_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.959881, 8.544370, 3.355472, 2.751420]
  full_rank: 4

lora_unet_double_blocks_6_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [16.407906, 10.730719, 8.145449, 4.889475]
  full_rank: 4

lora_unet_double_blocks_6_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.835155, 5.408069, 4.643528, 2.811406]
  full_rank: 4

lora_unet_double_blocks_6_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.362864, 1.294309, 0.934220, 0.889749]
  full_rank: 4

lora_unet_double_blocks_6_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.605736, 8.581130, 5.878630, 5.345479]
  full_rank: 4

lora_unet_double_blocks_6_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.560734, 2.483392, 1.289646, 0.957505]
  full_rank: 4

lora_unet_double_blocks_6_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.072804, 2.583861, 1.296324, 0.793923]
  full_rank: 4

lora_unet_double_blocks_6_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.346199, 3.735511, 2.206647, 1.242173]
  full_rank: 4

lora_unet_double_blocks_6_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.431595, 7.093750, 3.748124, 2.300254]
  full_rank: 4

lora_unet_double_blocks_6_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.308305, 2.819982, 0.961472, 0.549407]
  full_rank: 4

lora_unet_double_blocks_6_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.122735, 6.945949, 2.290096, 0.413046]
  full_rank: 4

lora_unet_double_blocks_6_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.912470, 5.143697, 2.834548, 2.520733]
  full_rank: 4

lora_unet_double_blocks_6_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.110312, 2.759887, 1.631676, 1.483106]
  full_rank: 4

lora_unet_double_blocks_6_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.248534, 1.231533, 0.782556, 0.622814]
  full_rank: 4

lora_unet_double_blocks_6_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.258369, 8.200759, 3.815901, 2.352312]
  full_rank: 4

lora_unet_double_blocks_7_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.252823, 4.391956, 3.702801, 1.734346]
  full_rank: 4

lora_unet_double_blocks_7_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.718159, 4.385240, 3.943101, 1.924467]
  full_rank: 4

lora_unet_double_blocks_7_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.174448, 3.685862, 1.976373, 1.677361]
  full_rank: 4

lora_unet_double_blocks_7_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.429450, 7.144499, 4.344018, 3.128155]
  full_rank: 4

lora_unet_double_blocks_7_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.981420, 4.540444, 2.149189, 1.005621]
  full_rank: 4

lora_unet_double_blocks_7_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.340734, 10.505915, 5.623682, 2.350622]
  full_rank: 4

lora_unet_double_blocks_7_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [16.975187, 10.714882, 6.750806, 4.031611]
  full_rank: 4

lora_unet_double_blocks_7_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.653961, 6.217373, 3.497015, 2.233098]
  full_rank: 4

lora_unet_double_blocks_7_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.168727, 1.275869, 0.863969, 0.599684]
  full_rank: 4

lora_unet_double_blocks_7_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.815516, 9.386750, 6.161304, 3.113893]
  full_rank: 4

lora_unet_double_blocks_7_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.165722, 3.732164, 1.939426, 1.204665]
  full_rank: 4

lora_unet_double_blocks_7_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.059416, 4.237646, 1.775669, 1.109954]
  full_rank: 4

lora_unet_double_blocks_7_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.421391, 3.429923, 1.692149, 0.933342]
  full_rank: 4

lora_unet_double_blocks_7_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.492082, 6.321487, 3.005689, 1.526631]
  full_rank: 4

lora_unet_double_blocks_7_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.240289, 2.473497, 1.531921, 0.796266]
  full_rank: 4

lora_unet_double_blocks_7_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.145745, 6.118687, 3.279461, 1.497744]
  full_rank: 4

lora_unet_double_blocks_7_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.012507, 5.862043, 5.012771, 2.843246]
  full_rank: 4

lora_unet_double_blocks_7_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.676799, 3.221111, 3.032137, 1.681537]
  full_rank: 4

lora_unet_double_blocks_7_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.512227, 0.661160, 0.531107, 0.505767]
  full_rank: 4

lora_unet_double_blocks_7_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.188707, 3.123296, 0.906535, 0.643738]
  full_rank: 4

lora_unet_double_blocks_8_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.711270, 5.522591, 3.785975, 2.709429]
  full_rank: 4

lora_unet_double_blocks_8_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.793524, 5.389561, 3.971217, 2.861399]
  full_rank: 4

lora_unet_double_blocks_8_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.436940, 3.585574, 2.624001, 1.684484]
  full_rank: 4

lora_unet_double_blocks_8_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.410421, 7.083336, 5.073008, 3.139512]
  full_rank: 4

lora_unet_double_blocks_8_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.185560, 4.372577, 3.464960, 2.832104]
  full_rank: 4

lora_unet_double_blocks_8_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.664125, 9.988829, 7.343461, 5.201818]
  full_rank: 4

lora_unet_double_blocks_8_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.472080, 7.887039, 6.381923, 4.432051]
  full_rank: 4

lora_unet_double_blocks_8_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.565884, 4.276446, 3.323223, 1.951319]
  full_rank: 4

lora_unet_double_blocks_8_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.690580, 1.326424, 0.839091, 0.692638]
  full_rank: 4

lora_unet_double_blocks_8_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.307419, 9.712711, 4.227709, 3.517931]
  full_rank: 4

lora_unet_double_blocks_8_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.435268, 3.808486, 2.219580, 1.182426]
  full_rank: 4

lora_unet_double_blocks_8_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.875803, 4.145278, 2.188027, 1.004147]
  full_rank: 4

lora_unet_double_blocks_8_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.175817, 4.459254, 2.819303, 1.319880]
  full_rank: 4

lora_unet_double_blocks_8_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.202967, 9.003147, 6.485180, 2.483173]
  full_rank: 4

lora_unet_double_blocks_8_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.557346, 1.270663, 1.082977, 0.817983]
  full_rank: 4

lora_unet_double_blocks_8_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.867531, 3.729418, 2.443427, 1.615908]
  full_rank: 4

lora_unet_double_blocks_8_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.945903, 7.117345, 6.540510, 4.492064]
  full_rank: 4

lora_unet_double_blocks_8_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.922822, 3.940777, 3.474309, 2.551971]
  full_rank: 4

lora_unet_double_blocks_8_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.480832, 1.149745, 0.940225, 0.552806]
  full_rank: 4

lora_unet_double_blocks_8_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.341854, 8.767846, 3.940828, 1.862580]
  full_rank: 4

lora_unet_double_blocks_9_img_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.913488, 5.069417, 4.641572, 4.044741]
  full_rank: 4

lora_unet_double_blocks_9_img_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.507413, 5.633067, 4.487589, 4.013282]
  full_rank: 4

lora_unet_double_blocks_9_img_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.709096, 4.503968, 3.590420, 2.141219]
  full_rank: 4

lora_unet_double_blocks_9_img_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.953665, 8.770853, 6.927496, 3.815624]
  full_rank: 4

lora_unet_double_blocks_9_img_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.715108, 4.834646, 4.563587, 2.608924]
  full_rank: 4

lora_unet_double_blocks_9_img_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.151666, 10.002100, 9.415634, 5.309959]
  full_rank: 4

lora_unet_double_blocks_9_img_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.544069, 9.903092, 7.636727, 6.715190]
  full_rank: 4

lora_unet_double_blocks_9_img_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.593723, 5.186786, 4.037712, 3.574656]
  full_rank: 4

lora_unet_double_blocks_9_img_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.357118, 0.732925, 0.554637, 0.515263]
  full_rank: 4

lora_unet_double_blocks_9_img_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.162018, 4.561627, 1.785449, 0.783236]
  full_rank: 4

lora_unet_double_blocks_9_txt_attn_proj.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.627312, 2.360839, 1.246429, 1.128333]
  full_rank: 4

lora_unet_double_blocks_9_txt_attn_proj.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.049547, 2.652935, 1.136104, 0.967558]
  full_rank: 4

lora_unet_double_blocks_9_txt_attn_qkv.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.067591, 2.575770, 1.656031, 1.103449]
  full_rank: 4

lora_unet_double_blocks_9_txt_attn_qkv.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.482775, 5.507210, 3.356112, 1.877694]
  full_rank: 4

lora_unet_double_blocks_9_txt_mlp_0.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.958414, 3.630098, 2.099621, 1.003460]
  full_rank: 4

lora_unet_double_blocks_9_txt_mlp_0.lora_up.weight:
  shape: torch.Size([12288, 4])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.129478, 9.477782, 5.402905, 2.217323]
  full_rank: 4

lora_unet_double_blocks_9_txt_mlp_2.lora_down.weight:
  shape: torch.Size([4, 12288])
  dtype: torch.float32
  parameters: 49152
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.311464, 7.395856, 6.192538, 4.104026]
  full_rank: 4

lora_unet_double_blocks_9_txt_mlp_2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.757896, 4.088594, 3.505376, 2.198760]
  full_rank: 4

lora_unet_double_blocks_9_txt_mod_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.017494, 0.636041, 0.531352, 0.506460]
  full_rank: 4

lora_unet_double_blocks_9_txt_mod_lin.lora_up.weight:
  shape: torch.Size([18432, 4])
  dtype: torch.float32
  parameters: 73728
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.576851, 2.611837, 0.907191, 0.392873]
  full_rank: 4

lora_unet_single_blocks_0_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.124989, 4.283694, 3.491138, 2.335686]
  full_rank: 4

lora_unet_single_blocks_0_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.561922, 13.640088, 11.012169, 7.250203]
  full_rank: 4

lora_unet_single_blocks_0_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [18.496147, 9.166124, 6.937630, 4.692323]
  full_rank: 4

lora_unet_single_blocks_0_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.741274, 4.373350, 3.363844, 2.252990]
  full_rank: 4

lora_unet_single_blocks_0_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.895605, 1.607031, 1.309338, 0.684538]
  full_rank: 4

lora_unet_single_blocks_0_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.501446, 6.803023, 5.501579, 2.729574]
  full_rank: 4

lora_unet_single_blocks_10_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.175624, 4.505432, 3.458061, 3.015589]
  full_rank: 4

lora_unet_single_blocks_10_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.769733, 15.596934, 11.066096, 9.408843]
  full_rank: 4

lora_unet_single_blocks_10_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.557505, 13.440234, 11.090173, 6.528519]
  full_rank: 4

lora_unet_single_blocks_10_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.548353, 6.327150, 5.546637, 3.238058]
  full_rank: 4

lora_unet_single_blocks_10_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.822083, 1.987350, 1.414417, 1.028319]
  full_rank: 4

lora_unet_single_blocks_10_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.697634, 8.651521, 4.557384, 3.883041]
  full_rank: 4

lora_unet_single_blocks_11_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.735783, 4.798132, 4.274716, 3.908251]
  full_rank: 4

lora_unet_single_blocks_11_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.796553, 14.359027, 13.857188, 11.524611]
  full_rank: 4

lora_unet_single_blocks_11_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.545171, 12.413641, 11.636280, 9.295906]
  full_rank: 4

lora_unet_single_blocks_11_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [5.946153, 5.644847, 5.425683, 4.663820]
  full_rank: 4

lora_unet_single_blocks_11_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.785515, 2.583364, 1.485399, 1.167705]
  full_rank: 4

lora_unet_single_blocks_11_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.593601, 10.738909, 5.471735, 3.993832]
  full_rank: 4

lora_unet_single_blocks_12_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.043909, 5.259016, 4.838033, 3.709340]
  full_rank: 4

lora_unet_single_blocks_12_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.137693, 15.665721, 14.355417, 11.203740]
  full_rank: 4

lora_unet_single_blocks_12_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.010245, 12.883468, 11.748906, 9.408053]
  full_rank: 4

lora_unet_single_blocks_12_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.581138, 5.852439, 5.062575, 4.647298]
  full_rank: 4

lora_unet_single_blocks_12_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.462745, 2.605869, 1.644576, 0.930260]
  full_rank: 4

lora_unet_single_blocks_12_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.123833, 9.729389, 5.875578, 3.389235]
  full_rank: 4

lora_unet_single_blocks_13_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.760628, 4.584238, 4.179081, 3.746678]
  full_rank: 4

lora_unet_single_blocks_13_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.058422, 13.952390, 13.049151, 10.968848]
  full_rank: 4

lora_unet_single_blocks_13_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [16.207140, 12.236588, 10.544902, 9.502688]
  full_rank: 4

lora_unet_single_blocks_13_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.752811, 5.733657, 4.664480, 4.574631]
  full_rank: 4

lora_unet_single_blocks_13_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.286573, 2.824482, 1.501141, 1.170051]
  full_rank: 4

lora_unet_single_blocks_13_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.558141, 8.619881, 5.067499, 3.370817]
  full_rank: 4

lora_unet_single_blocks_14_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.279532, 5.265913, 4.823796, 4.342260]
  full_rank: 4

lora_unet_single_blocks_14_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.090221, 16.340170, 14.291157, 12.181583]
  full_rank: 4

lora_unet_single_blocks_14_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.159331, 14.261066, 11.244794, 9.401973]
  full_rank: 4

lora_unet_single_blocks_14_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.313076, 6.170815, 5.269592, 4.495691]
  full_rank: 4

lora_unet_single_blocks_14_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.134667, 1.858436, 1.251118, 0.842381]
  full_rank: 4

lora_unet_single_blocks_14_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.589197, 6.245020, 5.020926, 3.846437]
  full_rank: 4

lora_unet_single_blocks_15_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.998858, 5.110216, 4.929862, 4.425727]
  full_rank: 4

lora_unet_single_blocks_15_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.693338, 15.970282, 13.687509, 12.848607]
  full_rank: 4

lora_unet_single_blocks_15_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.350019, 14.003579, 10.219590, 8.744693]
  full_rank: 4

lora_unet_single_blocks_15_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.145453, 5.867292, 4.370819, 3.936837]
  full_rank: 4

lora_unet_single_blocks_15_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.332220, 2.905719, 1.386326, 0.924184]
  full_rank: 4

lora_unet_single_blocks_15_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.279205, 9.065703, 4.424890, 3.675459]
  full_rank: 4

lora_unet_single_blocks_16_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.098833, 5.110235, 4.540781, 4.096303]
  full_rank: 4

lora_unet_single_blocks_16_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.725302, 15.336105, 14.606410, 11.251572]
  full_rank: 4

lora_unet_single_blocks_16_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.376076, 11.941166, 9.982471, 8.377249]
  full_rank: 4

lora_unet_single_blocks_16_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.071009, 5.225653, 4.424402, 3.942595]
  full_rank: 4

lora_unet_single_blocks_16_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.916077, 2.423492, 1.672517, 1.153247]
  full_rank: 4

lora_unet_single_blocks_16_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.596372, 10.275364, 5.743605, 4.926760]
  full_rank: 4

lora_unet_single_blocks_17_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.787256, 5.119752, 4.776235, 3.709126]
  full_rank: 4

lora_unet_single_blocks_17_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.444813, 15.151475, 13.888502, 10.627396]
  full_rank: 4

lora_unet_single_blocks_17_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.430033, 12.479344, 10.761836, 9.073329]
  full_rank: 4

lora_unet_single_blocks_17_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.079491, 5.375412, 4.718687, 3.982597]
  full_rank: 4

lora_unet_single_blocks_17_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.030429, 2.631472, 1.704175, 1.309784]
  full_rank: 4

lora_unet_single_blocks_17_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.165358, 8.138221, 7.308084, 5.673208]
  full_rank: 4

lora_unet_single_blocks_18_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.102972, 5.557861, 5.163772, 3.886007]
  full_rank: 4

lora_unet_single_blocks_18_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.903280, 16.153913, 13.787245, 11.089218]
  full_rank: 4

lora_unet_single_blocks_18_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [16.229206, 12.096176, 10.011867, 8.399747]
  full_rank: 4

lora_unet_single_blocks_18_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.498014, 5.570784, 4.286044, 3.971682]
  full_rank: 4

lora_unet_single_blocks_18_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.993956, 2.326919, 1.648516, 0.940571]
  full_rank: 4

lora_unet_single_blocks_18_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.066744, 8.876050, 7.265797, 4.559103]
  full_rank: 4

lora_unet_single_blocks_19_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.526790, 5.125596, 4.236030, 2.659078]
  full_rank: 4

lora_unet_single_blocks_19_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.258453, 16.169815, 13.475817, 7.692640]
  full_rank: 4

lora_unet_single_blocks_19_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.741641, 12.722312, 10.168428, 7.919898]
  full_rank: 4

lora_unet_single_blocks_19_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.570712, 5.317919, 4.721855, 3.701138]
  full_rank: 4

lora_unet_single_blocks_19_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.391376, 1.779913, 1.459768, 0.751682]
  full_rank: 4

lora_unet_single_blocks_19_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.639463, 7.732074, 6.215417, 2.426126]
  full_rank: 4

lora_unet_single_blocks_1_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.715216, 4.374234, 3.635781, 3.293145]
  full_rank: 4

lora_unet_single_blocks_1_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.517733, 13.904972, 11.831032, 10.567049]
  full_rank: 4

lora_unet_single_blocks_1_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.096655, 9.602702, 7.886052, 5.358217]
  full_rank: 4

lora_unet_single_blocks_1_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.133492, 4.590947, 3.957016, 2.681617]
  full_rank: 4

lora_unet_single_blocks_1_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.032153, 2.472591, 1.654955, 0.914745]
  full_rank: 4

lora_unet_single_blocks_1_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.127676, 8.033300, 6.465257, 4.762479]
  full_rank: 4

lora_unet_single_blocks_20_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.670004, 4.949429, 4.444438, 3.859935]
  full_rank: 4

lora_unet_single_blocks_20_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.528749, 15.015639, 14.490197, 12.306168]
  full_rank: 4

lora_unet_single_blocks_20_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [16.725260, 11.924251, 8.302004, 7.329999]
  full_rank: 4

lora_unet_single_blocks_20_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.934614, 5.303281, 3.933386, 3.638762]
  full_rank: 4

lora_unet_single_blocks_20_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.974208, 2.637237, 1.138145, 0.668625]
  full_rank: 4

lora_unet_single_blocks_20_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.283687, 8.114160, 5.663996, 2.665293]
  full_rank: 4

lora_unet_single_blocks_21_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.993847, 4.874687, 3.823385, 2.635853]
  full_rank: 4

lora_unet_single_blocks_21_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.090605, 17.853949, 11.341105, 7.217591]
  full_rank: 4

lora_unet_single_blocks_21_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [18.915501, 8.031943, 5.649131, 3.858521]
  full_rank: 4

lora_unet_single_blocks_21_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.581406, 3.790300, 2.819076, 2.027348]
  full_rank: 4

lora_unet_single_blocks_21_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.957465, 1.946241, 1.360813, 0.821665]
  full_rank: 4

lora_unet_single_blocks_21_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.918736, 8.539743, 5.379645, 3.883741]
  full_rank: 4

lora_unet_single_blocks_22_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.321198, 5.138086, 4.621944, 3.703753]
  full_rank: 4

lora_unet_single_blocks_22_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.827265, 16.538919, 13.473216, 9.823461]
  full_rank: 4

lora_unet_single_blocks_22_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [18.348434, 11.036833, 5.906477, 3.924020]
  full_rank: 4

lora_unet_single_blocks_22_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.308282, 5.098086, 2.989564, 2.144008]
  full_rank: 4

lora_unet_single_blocks_22_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.629753, 2.553498, 1.702094, 0.836080]
  full_rank: 4

lora_unet_single_blocks_22_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.716462, 7.079941, 5.572280, 3.491946]
  full_rank: 4

lora_unet_single_blocks_23_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.111937, 5.252055, 4.574143, 3.634169]
  full_rank: 4

lora_unet_single_blocks_23_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.203371, 18.601257, 11.666469, 10.584179]
  full_rank: 4

lora_unet_single_blocks_23_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [18.809906, 6.420301, 6.158155, 5.163371]
  full_rank: 4

lora_unet_single_blocks_23_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.746710, 2.999029, 2.989946, 2.583143]
  full_rank: 4

lora_unet_single_blocks_23_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.148150, 2.388544, 1.241338, 1.005486]
  full_rank: 4

lora_unet_single_blocks_23_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.445062, 7.516781, 6.700114, 4.766624]
  full_rank: 4

lora_unet_single_blocks_24_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.646947, 5.505529, 5.357764, 3.789466]
  full_rank: 4

lora_unet_single_blocks_24_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.189617, 16.209314, 14.840724, 11.370867]
  full_rank: 4

lora_unet_single_blocks_24_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.761801, 12.438972, 8.916774, 5.688728]
  full_rank: 4

lora_unet_single_blocks_24_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.201562, 5.339451, 4.111670, 2.681626]
  full_rank: 4

lora_unet_single_blocks_24_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.502218, 1.861480, 1.405377, 0.733003]
  full_rank: 4

lora_unet_single_blocks_24_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.269241, 8.509135, 4.400695, 2.845223]
  full_rank: 4

lora_unet_single_blocks_25_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.119442, 5.815145, 4.577015, 4.299452]
  full_rank: 4

lora_unet_single_blocks_25_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.132278, 15.561138, 15.269414, 12.107174]
  full_rank: 4

lora_unet_single_blocks_25_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [19.442392, 8.751592, 7.458963, 6.402833]
  full_rank: 4

lora_unet_single_blocks_25_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.410703, 4.214835, 3.643696, 3.175813]
  full_rank: 4

lora_unet_single_blocks_25_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.925156, 2.123771, 1.342796, 1.208583]
  full_rank: 4

lora_unet_single_blocks_25_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.322206, 9.107821, 5.140989, 4.301634]
  full_rank: 4

lora_unet_single_blocks_26_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.445251, 5.045485, 4.667500, 4.277866]
  full_rank: 4

lora_unet_single_blocks_26_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [22.066322, 15.586465, 12.717210, 10.699173]
  full_rank: 4

lora_unet_single_blocks_26_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [18.438419, 9.494494, 8.649781, 6.204408]
  full_rank: 4

lora_unet_single_blocks_26_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.104409, 4.629366, 4.105955, 3.030026]
  full_rank: 4

lora_unet_single_blocks_26_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.720033, 2.093938, 1.635820, 1.000980]
  full_rank: 4

lora_unet_single_blocks_26_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [14.462623, 7.977413, 4.512159, 4.227091]
  full_rank: 4

lora_unet_single_blocks_27_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.295472, 5.475245, 4.923184, 3.867825]
  full_rank: 4

lora_unet_single_blocks_27_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.717394, 16.916758, 13.364466, 10.465109]
  full_rank: 4

lora_unet_single_blocks_27_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [18.727283, 10.667245, 8.852567, 8.375286]
  full_rank: 4

lora_unet_single_blocks_27_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.844890, 4.710742, 4.252510, 3.918389]
  full_rank: 4

lora_unet_single_blocks_27_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.786030, 2.668766, 1.097743, 0.943742]
  full_rank: 4

lora_unet_single_blocks_27_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.562105, 8.982292, 5.597229, 4.129464]
  full_rank: 4

lora_unet_single_blocks_28_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.013216, 5.618413, 4.984480, 4.070669]
  full_rank: 4

lora_unet_single_blocks_28_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.728003, 16.342665, 13.409098, 11.573845]
  full_rank: 4

lora_unet_single_blocks_28_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [19.275066, 11.126874, 7.538828, 6.807518]
  full_rank: 4

lora_unet_single_blocks_28_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.558755, 4.973324, 3.667200, 3.321309]
  full_rank: 4

lora_unet_single_blocks_28_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.695678, 2.340302, 1.177779, 0.890971]
  full_rank: 4

lora_unet_single_blocks_28_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.453133, 7.550026, 5.789490, 3.313172]
  full_rank: 4

lora_unet_single_blocks_29_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.354233, 5.892999, 4.791717, 3.423902]
  full_rank: 4

lora_unet_single_blocks_29_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.598984, 16.324532, 13.814719, 8.667934]
  full_rank: 4

lora_unet_single_blocks_29_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [19.202467, 11.373932, 7.142605, 5.679035]
  full_rank: 4

lora_unet_single_blocks_29_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.515479, 4.929357, 3.501473, 2.922337]
  full_rank: 4

lora_unet_single_blocks_29_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.127458, 2.742430, 1.398279, 1.084972]
  full_rank: 4

lora_unet_single_blocks_29_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.345169, 7.925133, 4.481666, 4.046086]
  full_rank: 4

lora_unet_single_blocks_2_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.466612, 4.753036, 4.293318, 2.191895]
  full_rank: 4

lora_unet_single_blocks_2_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [18.596182, 14.380555, 13.082466, 7.009433]
  full_rank: 4

lora_unet_single_blocks_2_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.422422, 9.042139, 6.789959, 5.254172]
  full_rank: 4

lora_unet_single_blocks_2_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.560382, 4.371138, 3.066655, 2.607647]
  full_rank: 4

lora_unet_single_blocks_2_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.892463, 1.492999, 1.116846, 0.710641]
  full_rank: 4

lora_unet_single_blocks_2_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.553514, 7.361529, 4.897401, 2.258067]
  full_rank: 4

lora_unet_single_blocks_30_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.013866, 5.726118, 4.108595, 3.943535]
  full_rank: 4

lora_unet_single_blocks_30_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.890459, 16.919584, 12.114655, 10.842638]
  full_rank: 4

lora_unet_single_blocks_30_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [19.178032, 12.150067, 7.613638, 6.701227]
  full_rank: 4

lora_unet_single_blocks_30_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.348326, 5.208346, 3.720129, 3.392581]
  full_rank: 4

lora_unet_single_blocks_30_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.111402, 2.631491, 1.665238, 1.237545]
  full_rank: 4

lora_unet_single_blocks_30_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.432465, 9.220105, 5.929510, 5.137339]
  full_rank: 4

lora_unet_single_blocks_31_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.063500, 5.499420, 4.497316, 3.331636]
  full_rank: 4

lora_unet_single_blocks_31_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.138771, 18.093977, 12.542396, 9.240932]
  full_rank: 4

lora_unet_single_blocks_31_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [20.451303, 10.455669, 6.992821, 4.886266]
  full_rank: 4

lora_unet_single_blocks_31_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.044844, 4.860754, 3.365755, 2.521443]
  full_rank: 4

lora_unet_single_blocks_31_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.362272, 2.619415, 1.517872, 1.238434]
  full_rank: 4

lora_unet_single_blocks_31_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.155146, 6.412506, 5.791885, 4.500769]
  full_rank: 4

lora_unet_single_blocks_32_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [7.453941, 5.758077, 5.414575, 4.693275]
  full_rank: 4

lora_unet_single_blocks_32_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [22.067160, 18.025524, 13.258596, 12.229228]
  full_rank: 4

lora_unet_single_blocks_32_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [21.697750, 13.166266, 11.254758, 8.679232]
  full_rank: 4

lora_unet_single_blocks_32_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.270043, 5.486409, 5.217586, 4.281966]
  full_rank: 4

lora_unet_single_blocks_32_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.944315, 3.082365, 2.024707, 1.205451]
  full_rank: 4

lora_unet_single_blocks_32_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.245646, 11.033439, 5.066745, 2.953835]
  full_rank: 4

lora_unet_single_blocks_33_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.286036, 6.305237, 6.010829, 5.220493]
  full_rank: 4

lora_unet_single_blocks_33_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.624905, 17.663067, 15.403498, 14.040339]
  full_rank: 4

lora_unet_single_blocks_33_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [19.719463, 15.902328, 14.694723, 12.809082]
  full_rank: 4

lora_unet_single_blocks_33_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [8.326189, 6.819919, 6.707881, 5.941095]
  full_rank: 4

lora_unet_single_blocks_33_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.202373, 1.969752, 1.531351, 1.154361]
  full_rank: 4

lora_unet_single_blocks_33_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [15.313550, 9.436931, 4.320459, 2.833237]
  full_rank: 4

lora_unet_single_blocks_34_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.840703, 6.643075, 6.386405, 6.161012]
  full_rank: 4

lora_unet_single_blocks_34_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [22.298901, 20.474483, 16.981909, 13.975714]
  full_rank: 4

lora_unet_single_blocks_34_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [23.935226, 19.518103, 15.360977, 13.511180]
  full_rank: 4

lora_unet_single_blocks_34_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [10.071227, 8.877141, 7.902907, 7.066085]
  full_rank: 4

lora_unet_single_blocks_34_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [11.873669, 6.197742, 2.713848, 1.567615]
  full_rank: 4

lora_unet_single_blocks_34_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [16.646387, 11.676822, 7.621493, 3.562047]
  full_rank: 4

lora_unet_single_blocks_35_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.673382, 7.485374, 7.409205, 6.714885]
  full_rank: 4

lora_unet_single_blocks_35_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [21.977497, 20.459949, 18.510113, 15.491839]
  full_rank: 4

lora_unet_single_blocks_35_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [20.540817, 18.778612, 17.926771, 17.648594]
  full_rank: 4

lora_unet_single_blocks_35_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.498766, 10.171654, 9.757024, 9.702427]
  full_rank: 4

lora_unet_single_blocks_35_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [10.630827, 3.561755, 1.795764, 1.711386]
  full_rank: 4

lora_unet_single_blocks_35_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.325203, 11.131625, 5.214260, 2.938857]
  full_rank: 4

lora_unet_single_blocks_36_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.179798, 7.615706, 6.395960, 6.252482]
  full_rank: 4

lora_unet_single_blocks_36_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [22.318649, 20.406298, 18.120924, 17.333979]
  full_rank: 4

lora_unet_single_blocks_36_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [20.955694, 19.717440, 18.437319, 16.354771]
  full_rank: 4

lora_unet_single_blocks_36_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [10.374784, 8.380138, 6.902878, 6.132569]
  full_rank: 4

lora_unet_single_blocks_36_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [8.190230, 4.118485, 3.366460, 2.073222]
  full_rank: 4

lora_unet_single_blocks_36_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.820569, 9.919675, 7.148793, 4.341702]
  full_rank: 4

lora_unet_single_blocks_37_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.186532, 3.325453, 3.065218, 2.351569]
  full_rank: 4

lora_unet_single_blocks_37_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.825163, 11.711148, 10.736124, 8.402186]
  full_rank: 4

lora_unet_single_blocks_37_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [18.500456, 14.613473, 13.691038, 8.382496]
  full_rank: 4

lora_unet_single_blocks_37_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [5.546332, 4.257295, 3.455295, 2.391903]
  full_rank: 4

lora_unet_single_blocks_37_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [2.928175, 1.653083, 1.096867, 0.735952]
  full_rank: 4

lora_unet_single_blocks_37_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [9.377780, 4.999631, 3.520546, 2.484493]
  full_rank: 4

lora_unet_single_blocks_3_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.017918, 4.267921, 3.373669, 2.755239]
  full_rank: 4

lora_unet_single_blocks_3_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.489332, 15.514698, 9.382245, 7.845809]
  full_rank: 4

lora_unet_single_blocks_3_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.846401, 11.041682, 9.023951, 4.307630]
  full_rank: 4

lora_unet_single_blocks_3_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.105029, 5.215188, 4.342992, 2.144581]
  full_rank: 4

lora_unet_single_blocks_3_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [3.295433, 3.142233, 1.560848, 0.917825]
  full_rank: 4

lora_unet_single_blocks_3_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [10.230929, 9.260998, 7.082711, 3.783198]
  full_rank: 4

lora_unet_single_blocks_4_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.144995, 3.805719, 3.396871, 3.118772]
  full_rank: 4

lora_unet_single_blocks_4_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.000927, 12.328237, 11.874938, 10.252961]
  full_rank: 4

lora_unet_single_blocks_4_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.221041, 10.244337, 6.116925, 4.269309]
  full_rank: 4

lora_unet_single_blocks_4_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.788664, 4.874767, 3.172168, 2.179001]
  full_rank: 4

lora_unet_single_blocks_4_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.274591, 1.421066, 1.194520, 0.603023]
  full_rank: 4

lora_unet_single_blocks_4_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.589322, 5.702952, 5.048336, 1.743397]
  full_rank: 4

lora_unet_single_blocks_5_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.194190, 4.368820, 3.220998, 1.856220]
  full_rank: 4

lora_unet_single_blocks_5_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [20.340843, 14.390965, 9.862494, 5.673145]
  full_rank: 4

lora_unet_single_blocks_5_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.983027, 9.912253, 8.097104, 6.935187]
  full_rank: 4

lora_unet_single_blocks_5_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.029799, 5.127603, 4.035905, 3.674822]
  full_rank: 4

lora_unet_single_blocks_5_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.354441, 1.976859, 1.489463, 0.978684]
  full_rank: 4

lora_unet_single_blocks_5_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [12.343934, 6.849148, 5.776349, 4.330784]
  full_rank: 4

lora_unet_single_blocks_6_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.893118, 4.082341, 3.444949, 2.513122]
  full_rank: 4

lora_unet_single_blocks_6_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.079777, 14.136698, 10.957788, 8.173561]
  full_rank: 4

lora_unet_single_blocks_6_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.219234, 9.166494, 6.026339, 4.925928]
  full_rank: 4

lora_unet_single_blocks_6_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.912750, 4.530884, 3.210804, 2.593834]
  full_rank: 4

lora_unet_single_blocks_6_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.783479, 2.754796, 1.495962, 0.922444]
  full_rank: 4

lora_unet_single_blocks_6_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.883845, 8.705582, 5.785353, 4.159325]
  full_rank: 4

lora_unet_single_blocks_7_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [6.137379, 3.691693, 3.145633, 2.605585]
  full_rank: 4

lora_unet_single_blocks_7_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [19.848516, 13.151541, 10.619481, 8.703833]
  full_rank: 4

lora_unet_single_blocks_7_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [14.423022, 11.577823, 9.325877, 6.775421]
  full_rank: 4

lora_unet_single_blocks_7_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.898876, 5.518570, 4.566566, 3.129327]
  full_rank: 4

lora_unet_single_blocks_7_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [4.262565, 2.212750, 1.735657, 1.238111]
  full_rank: 4

lora_unet_single_blocks_7_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [11.623795, 8.907849, 7.213837, 4.640955]
  full_rank: 4

lora_unet_single_blocks_8_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.364961, 4.549510, 4.232813, 3.321447]
  full_rank: 4

lora_unet_single_blocks_8_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.281036, 15.401639, 13.975914, 10.330149]
  full_rank: 4

lora_unet_single_blocks_8_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [17.041058, 9.472842, 8.328014, 5.485138]
  full_rank: 4

lora_unet_single_blocks_8_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [7.649127, 4.864319, 4.424831, 2.922199]
  full_rank: 4

lora_unet_single_blocks_8_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.563782, 2.015668, 1.299587, 0.914731]
  full_rank: 4

lora_unet_single_blocks_8_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.545069, 6.554495, 5.263486, 3.383171]
  full_rank: 4

lora_unet_single_blocks_9_linear1.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.778300, 4.821252, 4.385724, 3.539418]
  full_rank: 4

lora_unet_single_blocks_9_linear1.lora_up.weight:
  shape: torch.Size([21504, 4])
  dtype: torch.float32
  parameters: 86016
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [17.450138, 15.178741, 14.439628, 10.840971]
  full_rank: 4

lora_unet_single_blocks_9_linear2.lora_down.weight:
  shape: torch.Size([4, 15360])
  dtype: torch.float32
  parameters: 61440
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [15.223135, 11.700624, 9.853961, 7.368674]
  full_rank: 4

lora_unet_single_blocks_9_linear2.lora_up.weight:
  shape: torch.Size([3072, 4])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [6.876796, 5.710847, 4.929435, 3.907387]
  full_rank: 4

lora_unet_single_blocks_9_modulation_lin.lora_down.weight:
  shape: torch.Size([4, 3072])
  dtype: torch.float32
  parameters: 12288
  inferred_rank: 4
  type: lora_down
  rank: 4
  singular_values: [5.583460, 1.885843, 0.909254, 0.627772]
  full_rank: 4

lora_unet_single_blocks_9_modulation_lin.lora_up.weight:
  shape: torch.Size([9216, 4])
  dtype: torch.float32
  parameters: 36864
  inferred_rank: 4
  type: lora_up
  rank: 4
  singular_values: [13.358402, 8.628783, 3.286651, 1.932036]
  full_rank: 4

================================================================================
LAYER STRUCTURE ANALYSIS
================================================================================
No standard layer structure found.

All unique key prefixes:
  lora_unet_double_blocks_0_img_attn_proj: 3 keys
  lora_unet_double_blocks_0_img_attn_qkv: 3 keys
  lora_unet_double_blocks_0_img_mlp_0: 3 keys
  lora_unet_double_blocks_0_img_mlp_2: 3 keys
  lora_unet_double_blocks_0_img_mod_lin: 3 keys
  lora_unet_double_blocks_0_txt_attn_proj: 3 keys
  lora_unet_double_blocks_0_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_0_txt_mlp_0: 3 keys
  lora_unet_double_blocks_0_txt_mlp_2: 3 keys
  lora_unet_double_blocks_0_txt_mod_lin: 3 keys
  lora_unet_double_blocks_10_img_attn_proj: 3 keys
  lora_unet_double_blocks_10_img_attn_qkv: 3 keys
  lora_unet_double_blocks_10_img_mlp_0: 3 keys
  lora_unet_double_blocks_10_img_mlp_2: 3 keys
  lora_unet_double_blocks_10_img_mod_lin: 3 keys
  lora_unet_double_blocks_10_txt_attn_proj: 3 keys
  lora_unet_double_blocks_10_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_10_txt_mlp_0: 3 keys
  lora_unet_double_blocks_10_txt_mlp_2: 3 keys
  lora_unet_double_blocks_10_txt_mod_lin: 3 keys
  lora_unet_double_blocks_11_img_attn_proj: 3 keys
  lora_unet_double_blocks_11_img_attn_qkv: 3 keys
  lora_unet_double_blocks_11_img_mlp_0: 3 keys
  lora_unet_double_blocks_11_img_mlp_2: 3 keys
  lora_unet_double_blocks_11_img_mod_lin: 3 keys
  lora_unet_double_blocks_11_txt_attn_proj: 3 keys
  lora_unet_double_blocks_11_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_11_txt_mlp_0: 3 keys
  lora_unet_double_blocks_11_txt_mlp_2: 3 keys
  lora_unet_double_blocks_11_txt_mod_lin: 3 keys
  lora_unet_double_blocks_12_img_attn_proj: 3 keys
  lora_unet_double_blocks_12_img_attn_qkv: 3 keys
  lora_unet_double_blocks_12_img_mlp_0: 3 keys
  lora_unet_double_blocks_12_img_mlp_2: 3 keys
  lora_unet_double_blocks_12_img_mod_lin: 3 keys
  lora_unet_double_blocks_12_txt_attn_proj: 3 keys
  lora_unet_double_blocks_12_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_12_txt_mlp_0: 3 keys
  lora_unet_double_blocks_12_txt_mlp_2: 3 keys
  lora_unet_double_blocks_12_txt_mod_lin: 3 keys
  lora_unet_double_blocks_13_img_attn_proj: 3 keys
  lora_unet_double_blocks_13_img_attn_qkv: 3 keys
  lora_unet_double_blocks_13_img_mlp_0: 3 keys
  lora_unet_double_blocks_13_img_mlp_2: 3 keys
  lora_unet_double_blocks_13_img_mod_lin: 3 keys
  lora_unet_double_blocks_13_txt_attn_proj: 3 keys
  lora_unet_double_blocks_13_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_13_txt_mlp_0: 3 keys
  lora_unet_double_blocks_13_txt_mlp_2: 3 keys
  lora_unet_double_blocks_13_txt_mod_lin: 3 keys
  lora_unet_double_blocks_14_img_attn_proj: 3 keys
  lora_unet_double_blocks_14_img_attn_qkv: 3 keys
  lora_unet_double_blocks_14_img_mlp_0: 3 keys
  lora_unet_double_blocks_14_img_mlp_2: 3 keys
  lora_unet_double_blocks_14_img_mod_lin: 3 keys
  lora_unet_double_blocks_14_txt_attn_proj: 3 keys
  lora_unet_double_blocks_14_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_14_txt_mlp_0: 3 keys
  lora_unet_double_blocks_14_txt_mlp_2: 3 keys
  lora_unet_double_blocks_14_txt_mod_lin: 3 keys
  lora_unet_double_blocks_15_img_attn_proj: 3 keys
  lora_unet_double_blocks_15_img_attn_qkv: 3 keys
  lora_unet_double_blocks_15_img_mlp_0: 3 keys
  lora_unet_double_blocks_15_img_mlp_2: 3 keys
  lora_unet_double_blocks_15_img_mod_lin: 3 keys
  lora_unet_double_blocks_15_txt_attn_proj: 3 keys
  lora_unet_double_blocks_15_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_15_txt_mlp_0: 3 keys
  lora_unet_double_blocks_15_txt_mlp_2: 3 keys
  lora_unet_double_blocks_15_txt_mod_lin: 3 keys
  lora_unet_double_blocks_16_img_attn_proj: 3 keys
  lora_unet_double_blocks_16_img_attn_qkv: 3 keys
  lora_unet_double_blocks_16_img_mlp_0: 3 keys
  lora_unet_double_blocks_16_img_mlp_2: 3 keys
  lora_unet_double_blocks_16_img_mod_lin: 3 keys
  lora_unet_double_blocks_16_txt_attn_proj: 3 keys
  lora_unet_double_blocks_16_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_16_txt_mlp_0: 3 keys
  lora_unet_double_blocks_16_txt_mlp_2: 3 keys
  lora_unet_double_blocks_16_txt_mod_lin: 3 keys
  lora_unet_double_blocks_17_img_attn_proj: 3 keys
  lora_unet_double_blocks_17_img_attn_qkv: 3 keys
  lora_unet_double_blocks_17_img_mlp_0: 3 keys
  lora_unet_double_blocks_17_img_mlp_2: 3 keys
  lora_unet_double_blocks_17_img_mod_lin: 3 keys
  lora_unet_double_blocks_17_txt_attn_proj: 3 keys
  lora_unet_double_blocks_17_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_17_txt_mlp_0: 3 keys
  lora_unet_double_blocks_17_txt_mlp_2: 3 keys
  lora_unet_double_blocks_17_txt_mod_lin: 3 keys
  lora_unet_double_blocks_18_img_attn_proj: 3 keys
  lora_unet_double_blocks_18_img_attn_qkv: 3 keys
  lora_unet_double_blocks_18_img_mlp_0: 3 keys
  lora_unet_double_blocks_18_img_mlp_2: 3 keys
  lora_unet_double_blocks_18_img_mod_lin: 3 keys
  lora_unet_double_blocks_18_txt_attn_proj: 3 keys
  lora_unet_double_blocks_18_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_18_txt_mlp_0: 3 keys
  lora_unet_double_blocks_18_txt_mlp_2: 3 keys
  lora_unet_double_blocks_18_txt_mod_lin: 3 keys
  lora_unet_double_blocks_1_img_attn_proj: 3 keys
  lora_unet_double_blocks_1_img_attn_qkv: 3 keys
  lora_unet_double_blocks_1_img_mlp_0: 3 keys
  lora_unet_double_blocks_1_img_mlp_2: 3 keys
  lora_unet_double_blocks_1_img_mod_lin: 3 keys
  lora_unet_double_blocks_1_txt_attn_proj: 3 keys
  lora_unet_double_blocks_1_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_1_txt_mlp_0: 3 keys
  lora_unet_double_blocks_1_txt_mlp_2: 3 keys
  lora_unet_double_blocks_1_txt_mod_lin: 3 keys
  lora_unet_double_blocks_2_img_attn_proj: 3 keys
  lora_unet_double_blocks_2_img_attn_qkv: 3 keys
  lora_unet_double_blocks_2_img_mlp_0: 3 keys
  lora_unet_double_blocks_2_img_mlp_2: 3 keys
  lora_unet_double_blocks_2_img_mod_lin: 3 keys
  lora_unet_double_blocks_2_txt_attn_proj: 3 keys
  lora_unet_double_blocks_2_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_2_txt_mlp_0: 3 keys
  lora_unet_double_blocks_2_txt_mlp_2: 3 keys
  lora_unet_double_blocks_2_txt_mod_lin: 3 keys
  lora_unet_double_blocks_3_img_attn_proj: 3 keys
  lora_unet_double_blocks_3_img_attn_qkv: 3 keys
  lora_unet_double_blocks_3_img_mlp_0: 3 keys
  lora_unet_double_blocks_3_img_mlp_2: 3 keys
  lora_unet_double_blocks_3_img_mod_lin: 3 keys
  lora_unet_double_blocks_3_txt_attn_proj: 3 keys
  lora_unet_double_blocks_3_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_3_txt_mlp_0: 3 keys
  lora_unet_double_blocks_3_txt_mlp_2: 3 keys
  lora_unet_double_blocks_3_txt_mod_lin: 3 keys
  lora_unet_double_blocks_4_img_attn_proj: 3 keys
  lora_unet_double_blocks_4_img_attn_qkv: 3 keys
  lora_unet_double_blocks_4_img_mlp_0: 3 keys
  lora_unet_double_blocks_4_img_mlp_2: 3 keys
  lora_unet_double_blocks_4_img_mod_lin: 3 keys
  lora_unet_double_blocks_4_txt_attn_proj: 3 keys
  lora_unet_double_blocks_4_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_4_txt_mlp_0: 3 keys
  lora_unet_double_blocks_4_txt_mlp_2: 3 keys
  lora_unet_double_blocks_4_txt_mod_lin: 3 keys
  lora_unet_double_blocks_5_img_attn_proj: 3 keys
  lora_unet_double_blocks_5_img_attn_qkv: 3 keys
  lora_unet_double_blocks_5_img_mlp_0: 3 keys
  lora_unet_double_blocks_5_img_mlp_2: 3 keys
  lora_unet_double_blocks_5_img_mod_lin: 3 keys
  lora_unet_double_blocks_5_txt_attn_proj: 3 keys
  lora_unet_double_blocks_5_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_5_txt_mlp_0: 3 keys
  lora_unet_double_blocks_5_txt_mlp_2: 3 keys
  lora_unet_double_blocks_5_txt_mod_lin: 3 keys
  lora_unet_double_blocks_6_img_attn_proj: 3 keys
  lora_unet_double_blocks_6_img_attn_qkv: 3 keys
  lora_unet_double_blocks_6_img_mlp_0: 3 keys
  lora_unet_double_blocks_6_img_mlp_2: 3 keys
  lora_unet_double_blocks_6_img_mod_lin: 3 keys
  lora_unet_double_blocks_6_txt_attn_proj: 3 keys
  lora_unet_double_blocks_6_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_6_txt_mlp_0: 3 keys
  lora_unet_double_blocks_6_txt_mlp_2: 3 keys
  lora_unet_double_blocks_6_txt_mod_lin: 3 keys
  lora_unet_double_blocks_7_img_attn_proj: 3 keys
  lora_unet_double_blocks_7_img_attn_qkv: 3 keys
  lora_unet_double_blocks_7_img_mlp_0: 3 keys
  lora_unet_double_blocks_7_img_mlp_2: 3 keys
  lora_unet_double_blocks_7_img_mod_lin: 3 keys
  lora_unet_double_blocks_7_txt_attn_proj: 3 keys
  lora_unet_double_blocks_7_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_7_txt_mlp_0: 3 keys
  lora_unet_double_blocks_7_txt_mlp_2: 3 keys
  lora_unet_double_blocks_7_txt_mod_lin: 3 keys
  lora_unet_double_blocks_8_img_attn_proj: 3 keys
  lora_unet_double_blocks_8_img_attn_qkv: 3 keys
  lora_unet_double_blocks_8_img_mlp_0: 3 keys
  lora_unet_double_blocks_8_img_mlp_2: 3 keys
  lora_unet_double_blocks_8_img_mod_lin: 3 keys
  lora_unet_double_blocks_8_txt_attn_proj: 3 keys
  lora_unet_double_blocks_8_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_8_txt_mlp_0: 3 keys
  lora_unet_double_blocks_8_txt_mlp_2: 3 keys
  lora_unet_double_blocks_8_txt_mod_lin: 3 keys
  lora_unet_double_blocks_9_img_attn_proj: 3 keys
  lora_unet_double_blocks_9_img_attn_qkv: 3 keys
  lora_unet_double_blocks_9_img_mlp_0: 3 keys
  lora_unet_double_blocks_9_img_mlp_2: 3 keys
  lora_unet_double_blocks_9_img_mod_lin: 3 keys
  lora_unet_double_blocks_9_txt_attn_proj: 3 keys
  lora_unet_double_blocks_9_txt_attn_qkv: 3 keys
  lora_unet_double_blocks_9_txt_mlp_0: 3 keys
  lora_unet_double_blocks_9_txt_mlp_2: 3 keys
  lora_unet_double_blocks_9_txt_mod_lin: 3 keys
  lora_unet_single_blocks_0_linear1: 3 keys
  lora_unet_single_blocks_0_linear2: 3 keys
  lora_unet_single_blocks_0_modulation_lin: 3 keys
  lora_unet_single_blocks_10_linear1: 3 keys
  lora_unet_single_blocks_10_linear2: 3 keys
  lora_unet_single_blocks_10_modulation_lin: 3 keys
  lora_unet_single_blocks_11_linear1: 3 keys
  lora_unet_single_blocks_11_linear2: 3 keys
  lora_unet_single_blocks_11_modulation_lin: 3 keys
  lora_unet_single_blocks_12_linear1: 3 keys
  lora_unet_single_blocks_12_linear2: 3 keys
  lora_unet_single_blocks_12_modulation_lin: 3 keys
  lora_unet_single_blocks_13_linear1: 3 keys
  lora_unet_single_blocks_13_linear2: 3 keys
  lora_unet_single_blocks_13_modulation_lin: 3 keys
  lora_unet_single_blocks_14_linear1: 3 keys
  lora_unet_single_blocks_14_linear2: 3 keys
  lora_unet_single_blocks_14_modulation_lin: 3 keys
  lora_unet_single_blocks_15_linear1: 3 keys
  lora_unet_single_blocks_15_linear2: 3 keys
  lora_unet_single_blocks_15_modulation_lin: 3 keys
  lora_unet_single_blocks_16_linear1: 3 keys
  lora_unet_single_blocks_16_linear2: 3 keys
  lora_unet_single_blocks_16_modulation_lin: 3 keys
  lora_unet_single_blocks_17_linear1: 3 keys
  lora_unet_single_blocks_17_linear2: 3 keys
  lora_unet_single_blocks_17_modulation_lin: 3 keys
  lora_unet_single_blocks_18_linear1: 3 keys
  lora_unet_single_blocks_18_linear2: 3 keys
  lora_unet_single_blocks_18_modulation_lin: 3 keys
  lora_unet_single_blocks_19_linear1: 3 keys
  lora_unet_single_blocks_19_linear2: 3 keys
  lora_unet_single_blocks_19_modulation_lin: 3 keys
  lora_unet_single_blocks_1_linear1: 3 keys
  lora_unet_single_blocks_1_linear2: 3 keys
  lora_unet_single_blocks_1_modulation_lin: 3 keys
  lora_unet_single_blocks_20_linear1: 3 keys
  lora_unet_single_blocks_20_linear2: 3 keys
  lora_unet_single_blocks_20_modulation_lin: 3 keys
  lora_unet_single_blocks_21_linear1: 3 keys
  lora_unet_single_blocks_21_linear2: 3 keys
  lora_unet_single_blocks_21_modulation_lin: 3 keys
  lora_unet_single_blocks_22_linear1: 3 keys
  lora_unet_single_blocks_22_linear2: 3 keys
  lora_unet_single_blocks_22_modulation_lin: 3 keys
  lora_unet_single_blocks_23_linear1: 3 keys
  lora_unet_single_blocks_23_linear2: 3 keys
  lora_unet_single_blocks_23_modulation_lin: 3 keys
  lora_unet_single_blocks_24_linear1: 3 keys
  lora_unet_single_blocks_24_linear2: 3 keys
  lora_unet_single_blocks_24_modulation_lin: 3 keys
  lora_unet_single_blocks_25_linear1: 3 keys
  lora_unet_single_blocks_25_linear2: 3 keys
  lora_unet_single_blocks_25_modulation_lin: 3 keys
  lora_unet_single_blocks_26_linear1: 3 keys
  lora_unet_single_blocks_26_linear2: 3 keys
  lora_unet_single_blocks_26_modulation_lin: 3 keys
  lora_unet_single_blocks_27_linear1: 3 keys
  lora_unet_single_blocks_27_linear2: 3 keys
  lora_unet_single_blocks_27_modulation_lin: 3 keys
  lora_unet_single_blocks_28_linear1: 3 keys
  lora_unet_single_blocks_28_linear2: 3 keys
  lora_unet_single_blocks_28_modulation_lin: 3 keys
  lora_unet_single_blocks_29_linear1: 3 keys
  lora_unet_single_blocks_29_linear2: 3 keys
  lora_unet_single_blocks_29_modulation_lin: 3 keys
  lora_unet_single_blocks_2_linear1: 3 keys
  lora_unet_single_blocks_2_linear2: 3 keys
  lora_unet_single_blocks_2_modulation_lin: 3 keys
  lora_unet_single_blocks_30_linear1: 3 keys
  lora_unet_single_blocks_30_linear2: 3 keys
  lora_unet_single_blocks_30_modulation_lin: 3 keys
  lora_unet_single_blocks_31_linear1: 3 keys
  lora_unet_single_blocks_31_linear2: 3 keys
  lora_unet_single_blocks_31_modulation_lin: 3 keys
  lora_unet_single_blocks_32_linear1: 3 keys
  lora_unet_single_blocks_32_linear2: 3 keys
  lora_unet_single_blocks_32_modulation_lin: 3 keys
  lora_unet_single_blocks_33_linear1: 3 keys
  lora_unet_single_blocks_33_linear2: 3 keys
  lora_unet_single_blocks_33_modulation_lin: 3 keys
  lora_unet_single_blocks_34_linear1: 3 keys
  lora_unet_single_blocks_34_linear2: 3 keys
  lora_unet_single_blocks_34_modulation_lin: 3 keys
  lora_unet_single_blocks_35_linear1: 3 keys
  lora_unet_single_blocks_35_linear2: 3 keys
  lora_unet_single_blocks_35_modulation_lin: 3 keys
  lora_unet_single_blocks_36_linear1: 3 keys
  lora_unet_single_blocks_36_linear2: 3 keys
  lora_unet_single_blocks_36_modulation_lin: 3 keys
  lora_unet_single_blocks_37_linear1: 3 keys
  lora_unet_single_blocks_37_linear2: 3 keys
  lora_unet_single_blocks_37_modulation_lin: 3 keys
  lora_unet_single_blocks_3_linear1: 3 keys
  lora_unet_single_blocks_3_linear2: 3 keys
  lora_unet_single_blocks_3_modulation_lin: 3 keys
  lora_unet_single_blocks_4_linear1: 3 keys
  lora_unet_single_blocks_4_linear2: 3 keys
  lora_unet_single_blocks_4_modulation_lin: 3 keys
  lora_unet_single_blocks_5_linear1: 3 keys
  lora_unet_single_blocks_5_linear2: 3 keys
  lora_unet_single_blocks_5_modulation_lin: 3 keys
  lora_unet_single_blocks_6_linear1: 3 keys
  lora_unet_single_blocks_6_linear2: 3 keys
  lora_unet_single_blocks_6_modulation_lin: 3 keys
  lora_unet_single_blocks_7_linear1: 3 keys
  lora_unet_single_blocks_7_linear2: 3 keys
  lora_unet_single_blocks_7_modulation_lin: 3 keys
  lora_unet_single_blocks_8_linear1: 3 keys
  lora_unet_single_blocks_8_linear2: 3 keys
  lora_unet_single_blocks_8_modulation_lin: 3 keys
  lora_unet_single_blocks_9_linear1: 3 keys
  lora_unet_single_blocks_9_linear2: 3 keys
  lora_unet_single_blocks_9_modulation_lin: 3 keys

================================================================================
MERGED LORA ANALYSIS
================================================================================
Checking attention weights for merged LoRA patterns...

Analyzing 152 attention weight matrices:

  1. lora_unet_double_blocks_0_img_attn_proj.lora_down.weight:
     Shape: torch.Size([4, 3072])
     Full rank: 4, Effective rank: 4
